{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "import sys\n",
    "sys.path.append('/home/ziniu.wzn/BayesCard')\n",
    "import pandas as pd\n",
    "import time\n",
    "import bz2\n",
    "import pickle\n",
    "import logging\n",
    "import ast\n",
    "\n",
    "from DeepDBUtils.ensemble_compilation.spn_ensemble import read_ensemble\n",
    "from DeepDBUtils.evaluation.utils import parse_query\n",
    "from DeepDBUtils.schemas.imdb.schema import gen_job_light_imdb_schema\n",
    "from DeepDBUtils.ensemble_compilation.graph_representation import Query\n",
    "from DeepDBUtils.ensemble_compilation.probabilistic_query import IndicatorExpectation, Expectation\n",
    "from DeepDBUtils.aqp_spn.aqp_spn import AQPSPN\n",
    "from DeepDBUtils.rspn.algorithms.ranges import NominalRange, NumericRange\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "from DeepDBUtils import ensemble_compilation, aqp_spn, rspn\n",
    "\n",
    "sys.modules['ensemble_compilation'] = ensemble_compilation\n",
    "sys.modules['aqp_spn'] = aqp_spn\n",
    "sys.modules['rspn'] = rspn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_single_query(range_conditions, factor, epsilon=0.1):\n",
    "    \n",
    "    query = dict()\n",
    "    fanout = []\n",
    "    col_name = list(factor.spn.column_names)\n",
    "    assert len(range_conditions) == 1\n",
    "    table_range = range_conditions[0]\n",
    "    assert len(table_range) == len(col_name)\n",
    "    for i, col in enumerate(table_range):\n",
    "        if isinstance(col, NumericRange):\n",
    "            if col.ranges[0][0] == col.ranges[0][1]:\n",
    "                query[col_name[i]] = col.ranges[0][0]\n",
    "            else:\n",
    "                inclusive = col.inclusive_intervals[0]\n",
    "                interval = []\n",
    "                if inclusive[0]:\n",
    "                    interval.append(col.ranges[0][0])\n",
    "                else:\n",
    "                    interval.append(col.ranges[0][0]+epsilon)\n",
    "                if inclusive[1]:\n",
    "                    interval.append(col.ranges[0][1])\n",
    "                else:\n",
    "                    interval.append(col.ranges[0][1]-epsilon)\n",
    "                query[col_name[i]] = interval\n",
    "        elif isinstance(col, NominalRange):\n",
    "            assert col.possible_values.size == 1\n",
    "            query[col_name[i]] = col.possible_values[0]\n",
    "\n",
    "    for table, f in factor.nominator_multipliers:\n",
    "        fanout.append(table+\".\"+f)\n",
    "\n",
    "\n",
    "    return query, fanout\n",
    "\n",
    "   \n",
    "\n",
    "def generate_factors(spn_ensemble, query, first_spn, next_mergeable_relationships, next_mergeable_tables,\n",
    "                     rdc_spn_selection=False, rdc_attribute_dict=None,\n",
    "                     merge_indicator_exp=True, exploit_overlapping=False,\n",
    "                     return_factor_values=False, exploit_incoming_multipliers=True,\n",
    "                     prefer_disjunct=False):\n",
    "    factors = []\n",
    "\n",
    "    # only operate on copy so that query object is not changed\n",
    "    # for greedy strategy it does not matter whether query is changed\n",
    "    # optimized version of:\n",
    "    # original_query = copy.deepcopy(query)\n",
    "    # query = copy.deepcopy(query)\n",
    "    original_query = query.copy_cardinality_query()\n",
    "    query = query.copy_cardinality_query()\n",
    "\n",
    "    # First SPN: Full_join_size*E(outgoing_mult * 1/multiplier * 1_{c_1 Λ… Λc_n})\n",
    "    # Again create auxilary query because intersection of query relationships and spn relationships\n",
    "    # is not necessarily a tree.\n",
    "    auxilary_query = Query(spn_ensemble.schema_graph)\n",
    "    for relationship in next_mergeable_relationships:\n",
    "        auxilary_query.add_join_condition(relationship)\n",
    "    auxilary_query.table_set.update(next_mergeable_tables)\n",
    "    auxilary_query.table_where_condition_dict = query.table_where_condition_dict\n",
    "\n",
    "    factors.append(first_spn.full_join_size)\n",
    "    conditions = first_spn.relevant_conditions(auxilary_query)\n",
    "    multipliers = first_spn.compute_multipliers(auxilary_query)\n",
    "\n",
    "    # E(1/multipliers * 1_{c_1 Λ… Λc_n})\n",
    "    expectation = IndicatorExpectation(multipliers, conditions, spn=first_spn, table_set=auxilary_query.table_set)\n",
    "    factors.append(expectation)\n",
    "\n",
    "    # mark tables as merged, remove merged relationships\n",
    "    merged_tables = next_mergeable_tables\n",
    "    query.relationship_set -= set(next_mergeable_relationships)\n",
    "\n",
    "    # remember which SPN was used to merge tables\n",
    "    corresponding_exp_dict = {}\n",
    "    for table in merged_tables:\n",
    "        corresponding_exp_dict[table] = expectation\n",
    "    extra_multplier_dict = {}\n",
    "\n",
    "    # merge subsequent relationships\n",
    "    while len(query.relationship_set) > 0:\n",
    "\n",
    "        # for next joins:\n",
    "        # if not exploit_overlapping: cardinality next subquery / next_neighbour.table_size\n",
    "\n",
    "        # compute set of next joinable neighbours\n",
    "        next_neighbours, neighbours_relationship_dict = spn_ensemble._next_neighbours(query, merged_tables)\n",
    "\n",
    "        # compute possible next merges and select greedily\n",
    "        next_spn, next_neighbour, next_mergeable_relationships = spn_ensemble._greedily_select_next_table(\n",
    "            original_query,\n",
    "            query,\n",
    "            next_neighbours,\n",
    "            exploit_overlapping,\n",
    "            merged_tables,\n",
    "            prefer_disjunct=prefer_disjunct,\n",
    "            rdc_spn_selection=rdc_spn_selection,\n",
    "            rdc_attribute_dict=rdc_attribute_dict)\n",
    "\n",
    "        # if outgoing: outgoing_mult appended to multipliers\n",
    "        relationship_to_neighbour = neighbours_relationship_dict[next_neighbour]\n",
    "        relationship_obj = spn_ensemble.schema_graph.relationship_dictionary[relationship_to_neighbour]\n",
    "\n",
    "        incoming_relationship = True\n",
    "        if relationship_obj.start == next_neighbour:\n",
    "            incoming_relationship = False\n",
    "            # outgoing relationship. Has to be included by E(outgoing_mult | C...)\n",
    "            if merge_indicator_exp:\n",
    "                # For this computation we simply add the multiplier to the respective indicator expectation.\n",
    "                end_table = relationship_obj.end\n",
    "                indicator_expectation_outgoing_spn = corresponding_exp_dict[end_table]\n",
    "                indicator_expectation_outgoing_spn.nominator_multipliers.append(\n",
    "                    (end_table, relationship_obj.multiplier_attribute_name))\n",
    "            else:\n",
    "                # E(outgoing_mult | C...) weighted by normalizing_multipliers\n",
    "                end_table = relationship_obj.end\n",
    "                feature = (end_table, relationship_obj.multiplier_attribute_name)\n",
    "\n",
    "                # Search SPN with maximal considered conditions\n",
    "                max_considered_where_conditions = -1\n",
    "                spn_for_exp_computation = None\n",
    "\n",
    "                for spn in spn_ensemble.spns:\n",
    "                    # attribute not even available\n",
    "                    if hasattr(spn, 'column_names'):\n",
    "                        if end_table + '.' + relationship_obj.multiplier_attribute_name not in spn.column_names:\n",
    "                            continue\n",
    "                    conditions = spn.relevant_conditions(original_query)\n",
    "                    if len(conditions) > max_considered_where_conditions:\n",
    "                        max_considered_where_conditions = len(conditions)\n",
    "                        spn_for_exp_computation = spn\n",
    "\n",
    "                assert spn_for_exp_computation is not None, \"No SPN found for expectation computation\"\n",
    "\n",
    "                # if spn_for_exp_computation is already used for outgoing multiplier computation it should be used\n",
    "                # again. This captures correlations of multipliers better.\n",
    "                if extra_multplier_dict.get(spn_for_exp_computation) is not None:\n",
    "                    expectation = extra_multplier_dict.get(spn_for_exp_computation)\n",
    "                    expectation.features.append(feature)\n",
    "                else:\n",
    "                    normalizing_multipliers = spn_for_exp_computation.compute_multipliers(original_query)\n",
    "                    conditions = spn_for_exp_computation.relevant_conditions(original_query)\n",
    "\n",
    "                    expectation = Expectation([feature], normalizing_multipliers, conditions,\n",
    "                                              spn=spn_for_exp_computation)\n",
    "                    extra_multplier_dict[spn_for_exp_computation] = expectation\n",
    "                    factors.append(expectation)\n",
    "\n",
    "        # remove relationship_to_neighbour from query\n",
    "        if relationship_to_neighbour in next_mergeable_relationships:\n",
    "            next_mergeable_relationships.remove(relationship_to_neighbour)\n",
    "        query.relationship_set.remove(relationship_to_neighbour)\n",
    "        merged_tables.add(next_neighbour)\n",
    "\n",
    "        # tables which are merged in the next step\n",
    "        next_merged_tables = spn_ensemble._merged_tables(next_mergeable_relationships)\n",
    "        next_merged_tables.add(next_neighbour)\n",
    "\n",
    "        # find overlapping relationships (relationships already merged that also appear in next_spn)\n",
    "        overlapping_relationships, overlapping_tables, no_overlapping_conditions = spn_ensemble._compute_overlap(\n",
    "            next_neighbour, query, original_query,\n",
    "            next_mergeable_relationships,\n",
    "            next_merged_tables,\n",
    "            next_spn)\n",
    "        # remove neighbour\n",
    "        overlapping_tables.remove(next_neighbour)\n",
    "\n",
    "        # do not ignore overlap. Exploit knowledge of overlap.\n",
    "        # in the computation use:\n",
    "        # correct_indicator_expectation_with_overlap/ indicator_expectation_of_overlap\n",
    "\n",
    "        # nominator query: indicator expectation of overlap + mergeable relationships\n",
    "        nominator_query = Query(spn_ensemble.schema_graph)\n",
    "        for relationship in overlapping_relationships:\n",
    "            nominator_query.add_join_condition(relationship)\n",
    "        for relationship in next_mergeable_relationships:\n",
    "            nominator_query.add_join_condition(relationship)\n",
    "        nominator_query.table_set.update(next_merged_tables)\n",
    "        nominator_query.table_where_condition_dict = query.table_where_condition_dict\n",
    "        conditions = next_spn.relevant_conditions(nominator_query,\n",
    "                                                  merged_tables=next_merged_tables.union(overlapping_tables))\n",
    "        multipliers = next_spn.compute_multipliers(nominator_query)\n",
    "\n",
    "        nominator_expectation = IndicatorExpectation(multipliers, conditions, spn=next_spn,\n",
    "                                                     table_set=next_merged_tables.union(overlapping_tables))\n",
    "\n",
    "        # we can still exploit the outgoing multiplier if the multiplier is present\n",
    "        if incoming_relationship and exploit_incoming_multipliers and len(overlapping_tables) == 0:\n",
    "            nominator_expectation.nominator_multipliers \\\n",
    "                .append((next_neighbour, relationship_obj.multiplier_attribute_name))\n",
    "\n",
    "        factors.append(nominator_expectation)\n",
    "\n",
    "        # denominator: indicator expectation of overlap\n",
    "        denominator_query = Query(spn_ensemble.schema_graph)\n",
    "        for relationship in overlapping_relationships:\n",
    "            denominator_query.add_join_condition(relationship)\n",
    "        denominator_query.table_set.update(next_merged_tables)\n",
    "        denominator_query.table_where_condition_dict = query.table_where_condition_dict\n",
    "\n",
    "        # constraints for next neighbor would not have any impact otherwise\n",
    "        conditions = next_spn.relevant_conditions(denominator_query, merged_tables=overlapping_tables)\n",
    "\n",
    "        next_neighbour_obj = spn_ensemble.schema_graph.table_dictionary[next_neighbour]\n",
    "        # add not null condition for next neighbor\n",
    "        conditions.append((next_neighbour, next_neighbour_obj.table_nn_attribute + \" IS NOT NULL\"))\n",
    "        multipliers = next_spn.compute_multipliers(denominator_query)\n",
    "        denominator_exp = IndicatorExpectation(multipliers, conditions, spn=next_spn, inverse=True,\n",
    "                                               table_set=overlapping_tables)\n",
    "\n",
    "        # we can still exploit the outgoing multiplier if the multiplier is present\n",
    "        if incoming_relationship and exploit_incoming_multipliers and len(overlapping_tables) == 0:\n",
    "            denominator_exp.nominator_multipliers \\\n",
    "                .append((next_neighbour, relationship_obj.multiplier_attribute_name))\n",
    "        factors.append(denominator_exp)\n",
    "\n",
    "        # mark tables as merged, remove merged relationships\n",
    "        for table in next_merged_tables:\n",
    "            merged_tables.add(table)\n",
    "            corresponding_exp_dict[table] = nominator_expectation\n",
    "\n",
    "        query.relationship_set -= set(next_mergeable_relationships)\n",
    "\n",
    "    return factors\n",
    "\n",
    "\n",
    "def factor_refine(factors_full):\n",
    "    factors_to_be_deleted = set()\n",
    "    for left_factor in factors_full:\n",
    "        if not isinstance(left_factor, IndicatorExpectation):\n",
    "            continue\n",
    "        for right_factor in factors_full:\n",
    "            if not isinstance(right_factor, IndicatorExpectation):\n",
    "                continue\n",
    "            if left_factor.is_inverse(right_factor):\n",
    "                factors_to_be_deleted.add(left_factor)\n",
    "                factors_to_be_deleted.add(right_factor)\n",
    "    return [factor for factor in factors_full if factor not in factors_to_be_deleted]\n",
    "\n",
    "def prepare_join_queries(ensemble_location, pairwise_rdc_path, query_filename,\n",
    "                         join_3_rdc_based, true_card_exist=False):\n",
    "    spn_ensemble = read_ensemble(ensemble_location, build_reverse_dict=True)\n",
    "\n",
    "    '''\n",
    "    set full join size for each spn manully\n",
    "    '''\n",
    "    if join_3_rdc_based:\n",
    "        spn_ensemble.spns[0].full_join_size = 38028991\n",
    "        spn_ensemble.spns[1].full_join_size = 70900181\n",
    "        spn_ensemble.spns[2].full_join_size = 14883333\n",
    "        spn_ensemble.spns[3].full_join_size = 3448422\n",
    "        spn_ensemble.spns[4].full_join_size = 4050205\n",
    "        spn_ensemble.spns[5].full_join_size = 36306324\n",
    "        spn_ensemble.spns[6].full_join_size = 6575448\n",
    "\n",
    "    parsed_queries = []\n",
    "\n",
    "    with open(pairwise_rdc_path, 'rb') as handle:\n",
    "        rdc_attribute_dict = pickle.load(handle)\n",
    "\n",
    "    schema = spn_ensemble.schema_graph\n",
    "    \n",
    "    true_card = []\n",
    "    with open(query_filename) as f:\n",
    "        queries = f.readlines()\n",
    "        for query_no, query_str in enumerate(queries):\n",
    "            if true_card_exist:\n",
    "                true_card.append(int(query_str.split(\"||\")[-1]))\n",
    "                query_str = query_str.split(\"||\")[0]\n",
    "            query_str = query_str.strip()\n",
    "\n",
    "            query = parse_query(query_str.strip(), schema)\n",
    "\n",
    "            first_spn, next_mergeable_relationships, next_mergeable_tables = \\\n",
    "                spn_ensemble._greedily_select_first_cardinality_spn(\n",
    "                    query, rdc_spn_selection=True, rdc_attribute_dict=rdc_attribute_dict)\n",
    "\n",
    "            factors = generate_factors(spn_ensemble, query, first_spn, next_mergeable_relationships,\n",
    "                                       next_mergeable_tables, rdc_spn_selection=True,\n",
    "                                       rdc_attribute_dict=rdc_attribute_dict, merge_indicator_exp=True,\n",
    "                                       exploit_incoming_multipliers=True, prefer_disjunct=False)\n",
    "\n",
    "            factors = factor_refine(factors)\n",
    "\n",
    "            parse_result = []\n",
    "            \n",
    "            for i, factor in enumerate(factors):\n",
    "                if isinstance(factor, IndicatorExpectation):\n",
    "                    assert isinstance(factor.spn, aqp_spn.aqp_spn.AQPSPN)\n",
    "                    range_conditions = factor.spn._parse_conditions(factor.conditions, group_by_columns=None,\n",
    "                                                                    group_by_tuples=None)\n",
    "                    \n",
    "                   \n",
    "                    actual_query, fanout = prepare_single_query(range_conditions, factor)\n",
    "\n",
    "                    parse_result.append({\"spn_index\": spn_ensemble.spns.index(factor.spn),\n",
    "                                         \"inverse\": factor.inverse,\n",
    "                                         \"query\": actual_query,\n",
    "                                         \"expectation\": fanout,\n",
    "                                         })\n",
    "                    \n",
    "                elif isinstance(factor, Expectation):\n",
    "                    raise NotImplementedError\n",
    "                else:\n",
    "                    parse_result.append(factor)\n",
    "\n",
    "            parsed_queries.append(parse_result)\n",
    "\n",
    "        return parsed_queries, true_card\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_location = \"/home/yuxing.hyx/repository/imdb-benchmark/spn_ensembles/ensemble_relationships_imdb-light_10000000.pkl\"\n",
    "query_filename = \"/home/ziniu.wzn/deepdb-public/benchmarks/job-light/sql/job_light_queries.sql\"\n",
    "pairwise_rdc_path = \"/home/yuxing.hyx/repository/imdb-benchmark/spn_ensembles/pairwise_rdc.pkl\"\n",
    "parsed_queries, true = prepare_join_queries(ensemble_location, pairwise_rdc_path, query_filename, \n",
    "                                      join_3_rdc_based=False, true_card_exist=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(parsed_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_queries[0][1]['range_conditions'][0][-1].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_single_query(range_conditions, factor, epsilon=0.1):\n",
    "    \n",
    "    query = dict()\n",
    "    fanout = []\n",
    "    col_name = list(factor.spn.column_names)\n",
    "    assert len(range_conditions) == 1\n",
    "    table_range = range_conditions[0]\n",
    "    assert len(table_range) == len(col_name)\n",
    "    for i, col in enumerate(table_range):\n",
    "        if isinstance(col, NumericRange):\n",
    "            if col.ranges[0][0] == col.ranges[0][1]:\n",
    "                query[col_name[i]] = col.ranges[0][0]\n",
    "            else:\n",
    "                inclusive = col.inclusive_intervals[0]\n",
    "                interval = []\n",
    "                if inclusive[0]:\n",
    "                    interval.append(col.ranges[0][0])\n",
    "                else:\n",
    "                    interval.append(col.ranges[0][0]+epsilon)\n",
    "                if inclusive[1]:\n",
    "                    interval.append(col.ranges[0][1])\n",
    "                else:\n",
    "                    interval.append(col.ranges[0][1]-epsilon)\n",
    "                query[col_name[i]] = interval\n",
    "        elif isinstance(col, NominalRange):\n",
    "            assert col.possible_values.size == 1\n",
    "            query[col_name[i]] = col.possible_values[0]\n",
    "\n",
    "    for table, f in factor.nominator_multipliers:\n",
    "        fanout.append(table+\".\"+f)\n",
    "\n",
    "    # assert range_fanout_lefts == range_fanout_rights\n",
    "\n",
    "    return query, fanout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_single_query(parsed_queries[0][1]['range_conditions'], parsed_queries[0][1]['factor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
