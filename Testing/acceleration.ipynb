{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from itertools import chain\n",
    "import sys\n",
    "sys.path.append('/Users/ziniuwu/Desktop/research/BayesCard')\n",
    "from Pgmpy.factors import factor_product\n",
    "from Pgmpy.models import BayesianModel, JunctionTree\n",
    "from Pgmpy.inference.EliminationOrder import (\n",
    "    WeightedMinFill,\n",
    "    MinNeighbors,\n",
    "    MinFill,\n",
    "    MinWeight,\n",
    ")\n",
    "from Pgmpy.factors.discrete import TabularCPD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import sys\n",
    "import copy\n",
    "from Models.pgmpy_BN import Pgmpy_BN\n",
    "from Testing.toy_dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat_attr1</th>\n",
       "      <th>cat_attr2</th>\n",
       "      <th>cat_attr3</th>\n",
       "      <th>cat_attr4</th>\n",
       "      <th>cat_attr5</th>\n",
       "      <th>cat_attr6</th>\n",
       "      <th>cat_attr7</th>\n",
       "      <th>cat_attr8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>8</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>29</td>\n",
       "      <td>36</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cat_attr1  cat_attr2  cat_attr3  cat_attr4  cat_attr5  cat_attr6  \\\n",
       "0          5          2          8          5          4          9   \n",
       "1          0          8          8          0         18         18   \n",
       "2          3          7         10          3          6          9   \n",
       "3          3          0          4          3          7         10   \n",
       "4          7          4         11          7         29         36   \n",
       "\n",
       "   cat_attr7  cat_attr8  \n",
       "0          2          7  \n",
       "1          8         56  \n",
       "2          8         19  \n",
       "3          0         34  \n",
       "4          4          4  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = toy_data_slightly_correlated_cat(nrows=100000, return_df=True)\n",
    "df = df.drop(\"id\", axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discretizing table takes 0.5326049327850342 secs\n",
      "Structure learning took 6.831698894500732 secs.\n",
      "done, parameter learning took 0.09351301193237305 secs.\n"
     ]
    }
   ],
   "source": [
    "BN = Pgmpy_BN('dmv')\n",
    "BN.build_from_data(df, algorithm=\"exact\", max_parents=3, ignore_cols=['id'], sample_size=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from itertools import chain\n",
    "import copy\n",
    "from Pgmpy.inference import Inference\n",
    "from Pgmpy.factors import factor_product\n",
    "from Pgmpy.models import BayesianModel, JunctionTree\n",
    "from Pgmpy.inference.EliminationOrder import (\n",
    "    WeightedMinFill,\n",
    "    MinNeighbors,\n",
    "    MinFill,\n",
    "    MinWeight,\n",
    ")\n",
    "from Pgmpy.factors.discrete import TabularCPD\n",
    "\n",
    "class VariableElimination(object):\n",
    "    def __init__(self, model, probs=None, root=True):\n",
    "        model.check_model()\n",
    "        self.model = model\n",
    "        if probs is not None:\n",
    "            self.probs = probs\n",
    "        else:\n",
    "            self.probs = dict()\n",
    "\n",
    "        if isinstance(model, JunctionTree):\n",
    "            self.variables = set(chain(*model.nodes()))\n",
    "        else:\n",
    "            self.variables = model.nodes()\n",
    "\n",
    "        self.cardinality = {}\n",
    "        self.factors = defaultdict(list)\n",
    "\n",
    "        if isinstance(model, BayesianModel):\n",
    "            self.state_names_map = {}\n",
    "            for node in model.nodes():\n",
    "                cpd = model.get_cpds(node)\n",
    "                if isinstance(cpd, TabularCPD):\n",
    "                    self.cardinality[node] = cpd.variable_card\n",
    "                    cpd = cpd.to_factor()\n",
    "                for var in cpd.scope():\n",
    "                    self.factors[var].append(cpd)\n",
    "                self.state_names_map.update(cpd.no_to_name)\n",
    "\n",
    "        elif isinstance(model, JunctionTree):\n",
    "            self.cardinality = model.get_cardinality()\n",
    "\n",
    "            for factor in model.get_factors():\n",
    "                for var in factor.variables:\n",
    "                    self.factors[var].append(factor)\n",
    "        if root:\n",
    "            self.root = self.get_root()\n",
    "\n",
    "    def get_root(self):\n",
    "        \"\"\"Returns the network's root node.\"\"\"\n",
    "\n",
    "        def find_root(graph, node):\n",
    "            predecessor = next(self.model.predecessors(node), None)\n",
    "            if predecessor:\n",
    "                root = find_root(graph, predecessor)\n",
    "            else:\n",
    "                root = node\n",
    "            return root\n",
    "\n",
    "        return find_root(self, list(self.model.nodes)[0])\n",
    "\n",
    "    def steiner_tree(self, nodes):\n",
    "        \"\"\"Returns the minimal part of the tree that contains a set of nodes.\"\"\"\n",
    "        sub_nodes = set()\n",
    "\n",
    "        def walk(node, path):\n",
    "            if len(nodes) == 0:\n",
    "                return\n",
    "\n",
    "            if node in nodes:\n",
    "                sub_nodes.update(path + [node])\n",
    "                nodes.remove(node)\n",
    "\n",
    "            for child in self.model.successors(node):\n",
    "                walk(child, path + [node])\n",
    "\n",
    "        walk(self.root, [])\n",
    "        sub_graph = self.model.subgraph(sub_nodes)\n",
    "        sub_graph.cardinalities = defaultdict(int)\n",
    "        for node in sub_graph.nodes:\n",
    "            sub_graph.cardinalities[node] = self.model.cardinalities[node]\n",
    "        return sub_graph\n",
    "\n",
    "    def get_probs(self, attribute, values):\n",
    "        \"\"\"\n",
    "        Calculate Pr(attr in values) where values must be a list\n",
    "        \"\"\"\n",
    "        factor = self.probs[attribute]\n",
    "        values = [factor.get_state_no(attribute, no) for no in values]\n",
    "        return np.sum(factor.values[values])\n",
    "\n",
    "\n",
    "    def _get_working_factors(self, variables=[], evidence=None, return_probs=False, reduce=True, BP=False):\n",
    "        \"\"\"\n",
    "        Uses the evidence given to the query methods to modify the factors before running\n",
    "        the variable elimination algorithm.\n",
    "        Parameters\n",
    "        ----------\n",
    "        evidence: dict\n",
    "            Dict of the form {variable: state}\n",
    "        Returns\n",
    "        -------\n",
    "        dict: Modified working factors.\n",
    "        \"\"\"\n",
    "        if BP:\n",
    "            working_factors = {\n",
    "                node: {(factor, None) for factor in self.factors[node]}\n",
    "                for node in self.factors\n",
    "            }\n",
    "            print(working_factors)\n",
    "            # Dealing with evidence. Reducing factors over it before VE is run.\n",
    "            if evidence:\n",
    "                for evidence_var in evidence:\n",
    "                    for factor, origin in working_factors[evidence_var]:\n",
    "                        factor_reduced = factor.reduce(\n",
    "                            [(evidence_var, evidence[evidence_var])], inplace=False\n",
    "                        )\n",
    "                        for var in factor_reduced.scope():\n",
    "                            if var in working_factors:\n",
    "                                working_factors[var].remove((factor, origin))\n",
    "                                working_factors[var].add((factor_reduced, evidence_var))\n",
    "                    if type(evidence[evidence_var]) != list:\n",
    "                        del working_factors[evidence_var]\n",
    "            print(working_factors)\n",
    "            return working_factors\n",
    "\n",
    "        else:\n",
    "            useful_var = copy.deepcopy(variables)\n",
    "            if evidence:\n",
    "                useful_var += list(evidence.keys())\n",
    "            sub_graph_model = self.steiner_tree(useful_var)\n",
    "            variables_sub_graph = set(sub_graph_model.nodes)\n",
    "\n",
    "            working_factors = dict()\n",
    "            for node in sub_graph_model.nodes:\n",
    "                working_factors[node] = set()\n",
    "                for factor in self.factors[node]:\n",
    "                    if set(factor.variables).issubset(variables_sub_graph):\n",
    "                        working_factors[node].add((factor, None))\n",
    "\n",
    "            if return_probs:\n",
    "                probs = dict()\n",
    "            # Dealing with evidence. Reducing factors over it before VE is run.\n",
    "            if evidence and reduce:\n",
    "                for evidence_var in evidence:\n",
    "                    for factor, origin in working_factors[evidence_var]:\n",
    "                        factor_reduced = factor.reduce(\n",
    "                            [(evidence_var, evidence[evidence_var])], inplace=False\n",
    "                        )\n",
    "                        if return_probs:\n",
    "                            factor_reduced.normalize()\n",
    "                            probs[evidence_var] = self.get_probs(evidence_var, evidence[evidence_var])\n",
    "                        for var in factor_reduced.scope():\n",
    "                            if var in working_factors:\n",
    "                                working_factors[var].remove((factor, origin))\n",
    "                                working_factors[var].add((factor_reduced, evidence_var))\n",
    "                    if type(evidence[evidence_var]) != list:\n",
    "                        del working_factors[evidence_var]\n",
    "            if return_probs:\n",
    "                return working_factors, sub_graph_model, probs\n",
    "            return working_factors, sub_graph_model\n",
    "\n",
    "    def _get_elimination_order(\n",
    "            self, variables=None, evidence=None, model=None, elimination_order=\"minfill\", show_progress=False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Deals with all elimination order parameters given to _variable_elimination method\n",
    "        and returns a list of variables that are to be eliminated\n",
    "        Parameters\n",
    "        ----------\n",
    "        elimination_order: str or list\n",
    "        Returns\n",
    "        -------\n",
    "        list: A list of variables names in the order they need to be eliminated.\n",
    "        \"\"\"\n",
    "        if model is None:\n",
    "            model = self.model\n",
    "        if isinstance(model, JunctionTree):\n",
    "            all_variables = set(chain(*model.nodes()))\n",
    "        else:\n",
    "            all_variables = model.nodes()\n",
    "\n",
    "        if variables is None:\n",
    "            to_eliminate = set(all_variables)\n",
    "        else:\n",
    "            not_evidence_eliminate = []\n",
    "            if evidence is not None:\n",
    "                for key in evidence:\n",
    "                    if type(evidence[key]) != list:\n",
    "                        not_evidence_eliminate.append(key)\n",
    "            to_eliminate = (\n",
    "                    set(all_variables)\n",
    "                    - set(variables)\n",
    "                    - set(not_evidence_eliminate)\n",
    "            )\n",
    "\n",
    "        # Step 1: If elimination_order is a list, verify it's correct and return.\n",
    "        if hasattr(elimination_order, \"__iter__\") and (\n",
    "                not isinstance(elimination_order, str)\n",
    "        ):\n",
    "            if any(\n",
    "                    var in elimination_order\n",
    "                    for var in set(variables).union(\n",
    "                        set(evidence.keys() if evidence else [])\n",
    "                    )\n",
    "            ):\n",
    "                raise ValueError(\n",
    "                    \"Elimination order contains variables which are in\"\n",
    "                    \" variables or evidence args\"\n",
    "                )\n",
    "            else:\n",
    "                return elimination_order\n",
    "\n",
    "        # Step 2: If elimination order is None or a Markov model, return a random order.\n",
    "        elif (elimination_order is None) or (not isinstance(model, BayesianModel)):\n",
    "            return to_eliminate\n",
    "\n",
    "        # Step 3: If elimination order is a str, compute the order using the specified heuristic.\n",
    "        elif isinstance(elimination_order, str) and isinstance(\n",
    "                model, BayesianModel\n",
    "        ):\n",
    "            heuristic_dict = {\n",
    "                \"weightedminfill\": WeightedMinFill,\n",
    "                \"minneighbors\": MinNeighbors,\n",
    "                \"minweight\": MinWeight,\n",
    "                \"minfill\": MinFill,\n",
    "            }\n",
    "            elimination_order = heuristic_dict[elimination_order.lower()](\n",
    "                model\n",
    "            ).get_elimination_order(nodes=to_eliminate, show_progress=show_progress)\n",
    "            return elimination_order\n",
    "\n",
    "    def _variable_elimination(\n",
    "            self,\n",
    "            variables,\n",
    "            operation,\n",
    "            evidence=None,\n",
    "            elimination_order=\"minfill\",\n",
    "            joint=True,\n",
    "            show_progress=False,\n",
    "            BP=False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Implementation of a generalized variable elimination.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        variables: list, array-like\n",
    "            variables that are not to be eliminated.\n",
    "\n",
    "        operation: str ('marginalize' | 'maximize')\n",
    "            The operation to do for eliminating the variable.\n",
    "\n",
    "        evidence: dict\n",
    "            a dict key, value pair as {var: state_of_var_observed}\n",
    "            None if no evidence\n",
    "\n",
    "        elimination_order: str or list (array-like)\n",
    "            If str: Heuristic to use to find the elimination order.\n",
    "            If array-like: The elimination order to use.\n",
    "            If None: A random elimination order is used.\n",
    "        \"\"\"\n",
    "        # Step 1: Deal with the input arguments.\n",
    "        if isinstance(variables, str):\n",
    "            raise TypeError(\"variables must be a list of strings\")\n",
    "        if isinstance(evidence, str):\n",
    "            raise TypeError(\"evidence must be a list of strings\")\n",
    "\n",
    "        # Dealing with the case when variables is not provided.\n",
    "        if not variables:\n",
    "            all_factors = []\n",
    "            for factor_li in self.factors.values():\n",
    "                all_factors.extend(factor_li)\n",
    "            if joint:\n",
    "                return factor_product(*set(all_factors))\n",
    "            else:\n",
    "                return set(all_factors)\n",
    "\n",
    "        # Step 2: Prepare data structures to run the algorithm.\n",
    "        eliminated_variables = set()\n",
    "        # Get working factors and elimination order\n",
    "        # tic = time.time()\n",
    "        if BP:\n",
    "            working_factors = self._get_working_factors(variables, evidence, BP=BP)\n",
    "            elimination_order = self._get_elimination_order(\n",
    "                variables, evidence, elimination_order=elimination_order, show_progress=show_progress\n",
    "            )\n",
    "        else:\n",
    "            working_factors, sub_graph_model = self._get_working_factors(variables, evidence)\n",
    "            elimination_order = self._get_elimination_order(\n",
    "                variables, evidence, sub_graph_model, elimination_order, show_progress=show_progress\n",
    "            )\n",
    "        # print(f\"getting elimination orders takes {time.time()-toc} secs\")\n",
    "        # Step 3: Run variable elimination\n",
    "        if show_progress:\n",
    "            pbar = tqdm(elimination_order)\n",
    "        else:\n",
    "            pbar = elimination_order\n",
    "        print(pbar)\n",
    "        for var in pbar:\n",
    "            #tic = time.time()\n",
    "            # print(var)\n",
    "            if show_progress:\n",
    "                pbar.set_description(\"Eliminating: {var}\".format(var=var))\n",
    "            # Removing all the factors containing the variables which are\n",
    "            # eliminated (as all the factors should be considered only once)\n",
    "            factors = [\n",
    "                factor\n",
    "                for factor, _ in working_factors[var]\n",
    "                if not set(factor.variables).intersection(eliminated_variables)\n",
    "            ]\n",
    "            phi = factor_product(*factors)\n",
    "            phi = getattr(phi, operation)([var], inplace=False)\n",
    "            del working_factors[var]\n",
    "            for variable in phi.variables:\n",
    "                if variable in working_factors:\n",
    "                    working_factors[variable].add((phi, var))\n",
    "            eliminated_variables.add(var)\n",
    "            # print(f\"eliminating {var} takes {time.time()-tic} secs\")\n",
    "\n",
    "        # Step 4: Prepare variables to be returned.\n",
    "        #tic = time.time()\n",
    "        final_distribution = set()\n",
    "        for node in working_factors:\n",
    "            for factor, origin in working_factors[node]:\n",
    "                if not set(factor.variables).intersection(eliminated_variables):\n",
    "                    final_distribution.add((factor, origin))\n",
    "        final_distribution = [factor for factor, _ in final_distribution]\n",
    "        # print(final_distribution)\n",
    "        # print(f\"the rest takes {time.time()-tic} secs\")\n",
    "        if joint:\n",
    "            if isinstance(self.model, BayesianModel):\n",
    "                return factor_product(*final_distribution).normalize(inplace=False)\n",
    "            else:\n",
    "                return factor_product(*final_distribution)\n",
    "        else:\n",
    "            query_var_factor = {}\n",
    "            for query_var in variables:\n",
    "                phi = factor_product(*final_distribution)\n",
    "                query_var_factor[query_var] = phi.marginalize(\n",
    "                    list(set(variables) - set([query_var])), inplace=False\n",
    "                ).normalize(inplace=False)\n",
    "            return query_var_factor\n",
    "\n",
    "    def query(\n",
    "            self,\n",
    "            variables,\n",
    "            evidence=None,\n",
    "            elimination_order=\"weightedminfill\",\n",
    "            joint=True,\n",
    "            show_progress=False,\n",
    "            BP=False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        variables: list\n",
    "            list of variables for which you want to compute the probability\n",
    "\n",
    "        evidence: dict\n",
    "            a dict key, value pair as {var: state_of_var_observed}\n",
    "            None if no evidence\n",
    "\n",
    "        elimination_order: list\n",
    "            order of variable eliminations (if nothing is provided) order is\n",
    "            computed automatically\n",
    "\n",
    "        joint: boolean (default: True)\n",
    "            If True, returns a Joint Distribution over `variables`.\n",
    "            If False, returns a dict of distributions over each of the `variables`.\n",
    "        \"\"\"\n",
    "        common_vars = set(evidence if evidence is not None else []).intersection(\n",
    "            set(variables)\n",
    "        )\n",
    "        if common_vars:\n",
    "            raise ValueError(\n",
    "                f\"Can't have the same variables in both `variables` and `evidence`. Found in both: {common_vars}\"\n",
    "            )\n",
    "\n",
    "        return self._variable_elimination(\n",
    "            variables=variables,\n",
    "            operation=\"marginalize\",\n",
    "            evidence=evidence,\n",
    "            elimination_order=elimination_order,\n",
    "            joint=joint,\n",
    "            show_progress=show_progress,\n",
    "            BP=BP\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeliefPropagation(Inference):\n",
    "    \"\"\"\n",
    "    Class for performing inference using Belief Propagation method.\n",
    "\n",
    "    Creates a Junction Tree or Clique Tree (JunctionTree class) for the input\n",
    "    probabilistic graphical model and performs calibration of the junction tree\n",
    "    so formed using belief propagation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: BayesianModel, MarkovModel, FactorGraph, JunctionTree\n",
    "        model for which inference is to performed\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model):\n",
    "        super(BeliefPropagation, self).__init__(model)\n",
    "\n",
    "        if not isinstance(model, JunctionTree):\n",
    "            self.junction_tree = model.to_junction_tree()\n",
    "        else:\n",
    "            self.junction_tree = copy.deepcopy(model)\n",
    "\n",
    "        self.clique_beliefs = {}\n",
    "        self.sepset_beliefs = {}\n",
    "\n",
    "    def get_cliques(self):\n",
    "        \"\"\"\n",
    "        Returns cliques used for belief propagation.\n",
    "        \"\"\"\n",
    "        return self.junction_tree.nodes()\n",
    "\n",
    "    def get_clique_beliefs(self):\n",
    "        \"\"\"\n",
    "        Returns clique beliefs. Should be called after the clique tree (or\n",
    "        junction tree) is calibrated.\n",
    "        \"\"\"\n",
    "        return self.clique_beliefs\n",
    "\n",
    "    def get_sepset_beliefs(self):\n",
    "        \"\"\"\n",
    "        Returns sepset beliefs. Should be called after clique tree (or junction\n",
    "        tree) is calibrated.\n",
    "        \"\"\"\n",
    "        return self.sepset_beliefs\n",
    "\n",
    "    def _update_beliefs(self, sending_clique, recieving_clique, operation):\n",
    "        \"\"\"\n",
    "        This is belief-update method.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        sending_clique: node (as the operation is on junction tree, node should be a tuple)\n",
    "            Node sending the message\n",
    "        recieving_clique: node (as the operation is on junction tree, node should be a tuple)\n",
    "            Node receiving the message\n",
    "        operation: str ('marginalize' | 'maximize')\n",
    "            The operation to do for passing messages between nodes.\n",
    "\n",
    "        Takes belief of one clique and uses it to update the belief of the\n",
    "        neighboring ones.\n",
    "        \"\"\"\n",
    "        sepset = frozenset(sending_clique).intersection(frozenset(recieving_clique))\n",
    "        sepset_key = frozenset((sending_clique, recieving_clique))\n",
    "\n",
    "        # \\sigma_{i \\rightarrow j} = \\sum_{C_i - S_{i, j}} \\beta_i\n",
    "        # marginalize the clique over the sepset\n",
    "        sigma = getattr(self.clique_beliefs[sending_clique], operation)(\n",
    "            list(frozenset(sending_clique) - sepset), inplace=False\n",
    "        )\n",
    "\n",
    "        # \\beta_j = \\beta_j * \\frac{\\sigma_{i \\rightarrow j}}{\\mu_{i, j}}\n",
    "        self.clique_beliefs[recieving_clique] *= (\n",
    "            sigma / self.sepset_beliefs[sepset_key]\n",
    "            if self.sepset_beliefs[sepset_key]\n",
    "            else sigma\n",
    "        )\n",
    "\n",
    "        # \\mu_{i, j} = \\sigma_{i \\rightarrow j}\n",
    "        self.sepset_beliefs[sepset_key] = sigma\n",
    "\n",
    "    def _is_converged(self, operation):\n",
    "        \"\"\"\n",
    "        Checks whether the calibration has converged or not. At convergence\n",
    "        the sepset belief would be precisely the sepset marginal.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        operation: str ('marginalize' | 'maximize')\n",
    "            The operation to do for passing messages between nodes.\n",
    "            if operation == marginalize, it checks whether the junction tree is calibrated or not\n",
    "            else if operation == maximize, it checks whether the junction tree is max calibrated or not\n",
    "\n",
    "        Formally, at convergence or at calibration this condition would be satisfied for\n",
    "\n",
    "        .. math:: \\sum_{C_i - S_{i, j}} \\beta_i = \\sum_{C_j - S_{i, j}} \\beta_j = \\mu_{i, j}\n",
    "\n",
    "        and at max calibration this condition would be satisfied\n",
    "\n",
    "        .. math:: \\max_{C_i - S_{i, j}} \\beta_i = \\max_{C_j - S_{i, j}} \\beta_j = \\mu_{i, j}\n",
    "        \"\"\"\n",
    "        # If no clique belief, then the clique tree is not calibrated\n",
    "        if not self.clique_beliefs:\n",
    "            return False\n",
    "\n",
    "        for edge in self.junction_tree.edges():\n",
    "            sepset = frozenset(edge[0]).intersection(frozenset(edge[1]))\n",
    "            sepset_key = frozenset(edge)\n",
    "            if (\n",
    "                edge[0] not in self.clique_beliefs\n",
    "                or edge[1] not in self.clique_beliefs\n",
    "                or sepset_key not in self.sepset_beliefs\n",
    "            ):\n",
    "                return False\n",
    "\n",
    "            marginal_1 = getattr(self.clique_beliefs[edge[0]], operation)(\n",
    "                list(frozenset(edge[0]) - sepset), inplace=False\n",
    "            )\n",
    "            marginal_2 = getattr(self.clique_beliefs[edge[1]], operation)(\n",
    "                list(frozenset(edge[1]) - sepset), inplace=False\n",
    "            )\n",
    "            if (\n",
    "                marginal_1 != marginal_2\n",
    "                or marginal_1 != self.sepset_beliefs[sepset_key]\n",
    "            ):\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def _calibrate_junction_tree(self, operation):\n",
    "        \"\"\"\n",
    "        Generalized calibration of junction tree or clique using belief propagation. This method can be used for both\n",
    "        calibrating as well as max-calibrating.\n",
    "        Uses Lauritzen-Spiegelhalter algorithm or belief-update message passing.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        operation: str ('marginalize' | 'maximize')\n",
    "            The operation to do for passing messages between nodes.\n",
    "\n",
    "        Reference\n",
    "        ---------\n",
    "        Algorithm 10.3 Calibration using belief propagation in clique tree\n",
    "        Probabilistic Graphical Models: Principles and Techniques\n",
    "        Daphne Koller and Nir Friedman.\n",
    "        \"\"\"\n",
    "        # Initialize clique beliefs as well as sepset beliefs\n",
    "        self.clique_beliefs = {\n",
    "            clique: self.junction_tree.get_factors(clique)\n",
    "            for clique in self.junction_tree.nodes()\n",
    "        }\n",
    "        self.sepset_beliefs = {\n",
    "            frozenset(edge): None for edge in self.junction_tree.edges()\n",
    "        }\n",
    "\n",
    "        for clique in self.junction_tree.nodes():\n",
    "            if not self._is_converged(operation=operation):\n",
    "                neighbors = self.junction_tree.neighbors(clique)\n",
    "                # update root's belief using nieighbor clique's beliefs\n",
    "                # upward pass\n",
    "                for neighbor_clique in neighbors:\n",
    "                    self._update_beliefs(neighbor_clique, clique, operation=operation)\n",
    "                bfs_edges = nx.algorithms.breadth_first_search.bfs_edges(\n",
    "                    self.junction_tree, clique\n",
    "                )\n",
    "                # update the beliefs of all the nodes starting from the root to leaves using root's belief\n",
    "                # downward pass\n",
    "                for edge in bfs_edges:\n",
    "                    self._update_beliefs(edge[0], edge[1], operation=operation)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    def calibrate(self):\n",
    "        \"\"\"\n",
    "        Calibration using belief propagation in junction tree or clique tree.\n",
    "        \"\"\"\n",
    "        self._calibrate_junction_tree(operation=\"marginalize\")\n",
    "\n",
    "    def max_calibrate(self):\n",
    "        \"\"\"\n",
    "        Max-calibration of the junction tree using belief propagation.\n",
    "        \"\"\"\n",
    "        self._calibrate_junction_tree(operation=\"maximize\")\n",
    "\n",
    "    def _query(\n",
    "        self, variables, operation, evidence=None, joint=True, show_progress=False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        This is a generalized query method that can be used for both query and map query.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        variables: list\n",
    "            list of variables for which you want to compute the probability\n",
    "        operation: str ('marginalize' | 'maximize')\n",
    "            The operation to do for passing messages between nodes.\n",
    "        evidence: dict\n",
    "            a dict key, value pair as {var: state_of_var_observed}\n",
    "            None if no evidence\n",
    "        References\n",
    "        ----------\n",
    "        Algorithm 10.4 Out-of-clique inference in clique tree\n",
    "        Probabilistic Graphical Models: Principles and Techniques Daphne Koller and Nir Friedman.\n",
    "        \"\"\"\n",
    "\n",
    "        is_calibrated = self._is_converged(operation=operation)\n",
    "        # Calibrate the junction tree if not calibrated\n",
    "        if not is_calibrated:\n",
    "            self.calibrate()\n",
    "\n",
    "        if not isinstance(variables, (list, tuple, set)):\n",
    "            query_variables = [variables]\n",
    "        else:\n",
    "            query_variables = list(variables)\n",
    "        query_variables.extend(evidence.keys() if evidence else [])\n",
    "\n",
    "        # Find a tree T' such that query_variables are a subset of scope(T')\n",
    "        nodes_with_query_variables = set()\n",
    "        for var in query_variables:\n",
    "            nodes_with_query_variables.update(\n",
    "                filter(lambda x: var in x, self.junction_tree.nodes())\n",
    "            )\n",
    "        subtree_nodes = nodes_with_query_variables\n",
    "\n",
    "        # Conversion of set to tuple just for indexing\n",
    "        nodes_with_query_variables = tuple(nodes_with_query_variables)\n",
    "        # As junction tree is a tree, that means that there would be only path between any two nodes in the tree\n",
    "        # thus we can just take the path between any two nodes; no matter there order is\n",
    "        for i in range(len(nodes_with_query_variables) - 1):\n",
    "            subtree_nodes.update(\n",
    "                nx.shortest_path(\n",
    "                    self.junction_tree,\n",
    "                    nodes_with_query_variables[i],\n",
    "                    nodes_with_query_variables[i + 1],\n",
    "                )\n",
    "            )\n",
    "        subtree_undirected_graph = self.junction_tree.subgraph(subtree_nodes)\n",
    "        # Converting subtree into a junction tree\n",
    "        if len(subtree_nodes) == 1:\n",
    "            subtree = JunctionTree()\n",
    "            subtree.add_node(subtree_nodes.pop())\n",
    "        else:\n",
    "            subtree = JunctionTree(subtree_undirected_graph.edges())\n",
    "\n",
    "        # Selecting a node is root node. Root node would be having only one neighbor\n",
    "        if len(subtree.nodes()) == 1:\n",
    "            root_node = list(subtree.nodes())[0]\n",
    "        else:\n",
    "            root_node = tuple(\n",
    "                filter(lambda x: len(list(subtree.neighbors(x))) == 1, subtree.nodes())\n",
    "            )[0]\n",
    "        clique_potential_list = [self.clique_beliefs[root_node]]\n",
    "\n",
    "        # For other nodes in the subtree compute the clique potentials as follows\n",
    "        # As all the nodes are nothing but tuples so simple set(root_node) won't work at it would update the set with'\n",
    "        # all the elements of the tuple; instead use set([root_node]) as it would include only the tuple not the\n",
    "        # internal elements within it.\n",
    "        parent_nodes = set([root_node])\n",
    "        nodes_traversed = set()\n",
    "        while parent_nodes:\n",
    "            parent_node = parent_nodes.pop()\n",
    "            for child_node in set(subtree.neighbors(parent_node)) - nodes_traversed:\n",
    "                clique_potential_list.append(\n",
    "                    self.clique_beliefs[child_node]\n",
    "                    / self.sepset_beliefs[frozenset([parent_node, child_node])]\n",
    "                )\n",
    "                parent_nodes.update([child_node])\n",
    "            nodes_traversed.update([parent_node])\n",
    "\n",
    "        # Add factors to the corresponding junction tree\n",
    "        subtree.add_factors(*clique_potential_list)\n",
    "\n",
    "        # Sum product variable elimination on the subtree\n",
    "        print(f\"nodes: {subtree.nodes}\")\n",
    "        print(f\"edges: {subtree.edges}\")\n",
    "        variable_elimination = VariableElimination(subtree, root=False)\n",
    "        if operation == \"marginalize\":\n",
    "            return variable_elimination.query(\n",
    "                variables=variables,\n",
    "                evidence=evidence,\n",
    "                joint=joint,\n",
    "                show_progress=show_progress,\n",
    "                BP=True\n",
    "            )\n",
    "        elif operation == \"maximize\":\n",
    "            return variable_elimination.map_query(\n",
    "                variables=variables, evidence=evidence, show_progress=show_progress\n",
    "            )\n",
    "\n",
    "    def query(self, variables, evidence=None, joint=True, show_progress=False):\n",
    "        \"\"\"\n",
    "        Query method using belief propagation.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        variables: list\n",
    "            list of variables for which you want to compute the probability\n",
    "\n",
    "        evidence: dict\n",
    "            a dict key, value pair as {var: state_of_var_observed}\n",
    "            None if no evidence\n",
    "\n",
    "        joint: boolean\n",
    "            If True, returns a Joint Distribution over `variables`.\n",
    "            If False, returns a dict of distributions over each of the `variables`.\n",
    "        \"\"\"\n",
    "        common_vars = set(evidence if evidence is not None else []).intersection(\n",
    "            set(variables)\n",
    "        )\n",
    "        if common_vars:\n",
    "            raise ValueError(\n",
    "                f\"Can't have the same variables in both `variables` and `evidence`. Found in both: {common_vars}\"\n",
    "            )\n",
    "\n",
    "        result = self._query(\n",
    "            variables=variables,\n",
    "            operation=\"marginalize\",\n",
    "            evidence=evidence,\n",
    "            joint=joint,\n",
    "            show_progress=show_progress,\n",
    "        )\n",
    "        if joint:\n",
    "            return result.normalize(inplace=False)\n",
    "        else:\n",
    "            return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OutEdgeView([('cat_attr4', 'cat_attr1'), ('cat_attr4', 'cat_attr6'), ('cat_attr1', 'cat_attr3'), ('cat_attr7', 'cat_attr2'), ('cat_attr2', 'cat_attr3'), ('cat_attr5', 'cat_attr6')])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BN.model.edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ve = VariableElimination(BN.model)\n",
    "BN.infer_algo = \"exact\"\n",
    "BN.infer_machine = ve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_graph(model, nodes, root):\n",
    "    \"\"\"Returns the minimal part of the tree that contains a set of nodes.\"\"\"\n",
    "    sub_nodes = set()\n",
    "\n",
    "    def walk(node, path):\n",
    "        if len(nodes) == 0:\n",
    "            return\n",
    "\n",
    "        if node in nodes:\n",
    "            sub_nodes.update(path + [node])\n",
    "            nodes.remove(node)\n",
    "\n",
    "        for child in model.successors(node):\n",
    "            print(child)\n",
    "            walk(child, path + [node])\n",
    "\n",
    "    walk(root, [])\n",
    "    sub_graph = model.subgraph(sub_nodes)\n",
    "    sub_graph.cardinalities = defaultdict(int)\n",
    "    for node in sub_graph.nodes:\n",
    "        sub_graph.cardinalities[node] = model.cardinalities[node]\n",
    "    return sub_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cat_attr4'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ve.root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat_attr1\n",
      "cat_attr3\n",
      "cat_attr6\n",
      "['cat_attr4', 'cat_attr1']\n",
      "[('cat_attr4', 'cat_attr1')]\n"
     ]
    }
   ],
   "source": [
    "m = steiner_tree(BN.model, ['cat_attr1', 'cat_attr5'], ve.root)\n",
    "print(m.nodes)\n",
    "print(m.edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'cat_attr5'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-7bc8be9993ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mve\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_working_factors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cat_attr1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'cat_attr5'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-30-4c118700bf97>\u001b[0m in \u001b[0;36m_get_working_factors\u001b[0;34m(self, variables, evidence, return_probs, reduce, BP)\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mevidence\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mevidence_var\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mevidence\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mfactor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morigin\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mworking_factors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mevidence_var\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m                         factor_reduced = factor.reduce(\n\u001b[1;32m    156\u001b[0m                             \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevidence_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevidence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mevidence_var\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'cat_attr5'"
     ]
    }
   ],
   "source": [
    "f = ve._get_working_factors(['cat_attr1'], {'cat_attr5': [0, 1, 2, 3]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp = BeliefPropagation(BN.model)\n",
    "BN.infer_algo = \"BP\"\n",
    "BN.infer_machine = bp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'cat_attr5'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-292b5c8310fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'cat_attr1'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cat_attr5'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/research/BayesCard/Models/pgmpy_BN.py\u001b[0m in \u001b[0;36mquery\u001b[0;34m(self, query, num_samples, n_distinct, coverage, return_prob, sample_size)\u001b[0m\n\u001b[1;32m    346\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m                     \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m                     \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_machine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevidence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"attr: {attr}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"evidence: {query}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-4c118700bf97>\u001b[0m in \u001b[0;36mquery\u001b[0;34m(self, variables, evidence, elimination_order, joint, show_progress, BP)\u001b[0m\n\u001b[1;32m    393\u001b[0m             \u001b[0mjoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0mshow_progress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshow_progress\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m             \u001b[0mBP\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m         )\n",
      "\u001b[0;32m<ipython-input-30-4c118700bf97>\u001b[0m in \u001b[0;36m_variable_elimination\u001b[0;34m(self, variables, operation, evidence, elimination_order, joint, show_progress, BP)\u001b[0m\n\u001b[1;32m    295\u001b[0m             )\n\u001b[1;32m    296\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m             \u001b[0mworking_factors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_graph_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_working_factors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevidence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m             elimination_order = self._get_elimination_order(\n\u001b[1;32m    299\u001b[0m                 \u001b[0mvariables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevidence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_graph_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melimination_order\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_progress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshow_progress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-4c118700bf97>\u001b[0m in \u001b[0;36m_get_working_factors\u001b[0;34m(self, variables, evidence, return_probs, reduce, BP)\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mevidence\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mevidence_var\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mevidence\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mfactor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morigin\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mworking_factors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mevidence_var\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m                         factor_reduced = factor.reduce(\n\u001b[1;32m    156\u001b[0m                             \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevidence_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevidence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mevidence_var\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'cat_attr5'"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "a = BN.query({'cat_attr1':[0,1,2,3], 'cat_attr5': [0, 1, 2, 3]})\n",
    "print(time.time()-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9814.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9814"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.query(\"cat_attr1 in [0,1,2,3] and cat_attr5 in [0, 1, 2, 3]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import jit\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "x = np.arange(100).reshape(10, 10)\n",
    "\n",
    "class exper:\n",
    "    def __init__(self):\n",
    "        self.trace = None\n",
    "        \n",
    "    @jit\n",
    "    def go_fast(self, a): # Function is compiled and runs in machine code\n",
    "        trace = None\n",
    "        for i in range(a.shape[0]):\n",
    "            if trace is None:\n",
    "                trace = 0.0\n",
    "            trace += np.tanh(a[i, i])\n",
    "        return a + trace\n",
    "\n",
    "# DO NOT REPORT THIS... COMPILATION TIME IS INCLUDED IN THE EXECUTION TIME!\n",
    "e = exper()\n",
    "start = time.time()\n",
    "e.go_fast(x)\n",
    "end = time.time()\n",
    "print(\"Elapsed (with compilation) = %s\" % (end - start))\n",
    "\n",
    "# NOW THE FUNCTION IS COMPILED, RE-TIME IT EXECUTING FROM CACHE\n",
    "start = time.time()\n",
    "e.go_fast(x)\n",
    "end = time.time()\n",
    "print(\"Elapsed (after compilation) = %s\" % (end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class exper:\n",
    "    def __init__(self):\n",
    "        self.trace = None\n",
    "        \n",
    "    def go_fast(self, a): # Function is compiled and runs in machine code\n",
    "        trace = None\n",
    "        for i in range(a.shape[0]):\n",
    "            if trace is None:\n",
    "                trace = 0.0\n",
    "            trace += np.tanh(a[i, i])\n",
    "        return a + trace\n",
    "\n",
    "# DO NOT REPORT THIS... COMPILATION TIME IS INCLUDED IN THE EXECUTION TIME!\n",
    "e = exper()\n",
    "start = time.time()\n",
    "e.go_fast(x)\n",
    "end = time.time()\n",
    "print(\"Elapsed (with compilation) = %s\" % (end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
