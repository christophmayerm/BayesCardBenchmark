{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pareto\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.append('/home/ziniu.wzn/BayesCard')\n",
    "from Models.Bayescard_BN import Bayescard_BN\n",
    "from time import perf_counter\n",
    "from Evaluation.utils import parse_query\n",
    "from Evaluation.cardinality_estimation import parse_query_single_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize_series(s, domain_size):\n",
    "    n_invalid = len(s[s>=domain_size])\n",
    "    s = s[s<domain_size]\n",
    "    s = np.floor(s)\n",
    "    new_s = np.random.randint(domain_size, size=n_invalid)\n",
    "    s = np.concatenate((s, new_s))\n",
    "    return np.random.permutation(s)\n",
    "    \n",
    "def data_generation(skew, domain_size, correlation, column_size, nrows=1000000):\n",
    "    data = np.zeros((column_size, nrows))\n",
    "    for i in range(column_size):\n",
    "        if i == 0:\n",
    "            s = np.random.randint(domain_size, size=nrows)\n",
    "            data[i,:] = s\n",
    "            continue\n",
    "        s = pareto.rvs(b=skew, scale=1, size=nrows)\n",
    "        s = discretize_series(s, domain_size)\n",
    "        if i == 1:\n",
    "            selected_cols = [0]\n",
    "        else:\n",
    "            num_selected_cols = max(np.random.randint(int(np.ceil(i*0.1))), 1)\n",
    "            selected_cols = np.random.permutation(i)[0:num_selected_cols]\n",
    "        idx = np.random.permutation(nrows)[0:int(nrows*correlation)]\n",
    "        if len(idx) != 0:\n",
    "            selected_data = data[selected_cols, :]\n",
    "            selected_data = np.ceil(np.mean(selected_data, axis=0))\n",
    "            s[idx] = selected_data[idx]\n",
    "        assert len(np.unique(s)) <= domain_size, \"invalid domain\"\n",
    "        data[i,:] = s\n",
    "        \n",
    "    data = pd.DataFrame(data=data.transpose(), columns=[f\"attr{i}\" for i in range(column_size)])\n",
    "    return data\n",
    "\n",
    "def query_generation(data, table_name, num_sample=200, p=0.8, nval_per_col=4, skip_zero_bit=6):\n",
    "    queries = []\n",
    "    cards = []\n",
    "    for i in range(num_sample):\n",
    "        query, card = generate_single_query(data, table_name, p, nval_per_col, skip_zero_bit)\n",
    "        while query is None:\n",
    "            query, card = generate_single_query(data, table_name, p, nval_per_col, skip_zero_bit)\n",
    "        queries.append(query)\n",
    "        cards.append(card)\n",
    "    return queries, cards\n",
    "\n",
    "def generate_single_query(df, table_name, p=0.8, nval_per_col=4, skip_zero_bit=6):\n",
    "    query = f\"SELECT COUNT(*) FROM {table_name} WHERE \"\n",
    "    execute_query = \"\"\n",
    "    column_names = df.columns\n",
    "    n_cols = 0\n",
    "    for i, col in enumerate(column_names):\n",
    "        a = np.random.choice([0,1], p=[p,1-p])\n",
    "        if a == 0:\n",
    "            index = np.random.choice(len(df), size=nval_per_col)\n",
    "            val = sorted(list(df[col].iloc[index]))\n",
    "            left_val = val[0]\n",
    "            right_val = val[-1]\n",
    "            if left_val == right_val:\n",
    "                sub_query = col + '==' + str(left_val) + ' and '\n",
    "                act_sub_query = col + ' = ' + str(left_val) + ' AND '\n",
    "            else:\n",
    "                if skip_zero_bit:\n",
    "                    left_val += skip_zero_bit\n",
    "                    right_val += skip_zero_bit\n",
    "                sub_query = str(left_val) + ' <= ' + col + ' <= ' + str(right_val) + ' and '\n",
    "                act_sub_query = col + ' >= ' + str(left_val) + ' AND ' + col + ' <= ' + str(right_val) + ' AND '\n",
    "            execute_query += sub_query\n",
    "            query += act_sub_query\n",
    "    if execute_query == \"\":\n",
    "        return None,  None\n",
    "    execute_query = execute_query[:-5]\n",
    "    query = query[:-5]\n",
    "    card = len(df.query(execute_query))\n",
    "    if card==0:\n",
    "        return None, None\n",
    "    return query, card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one(skew, domain_size, correlation, column_size, nrows=1000000, num_sample=200, \n",
    "              p=0.8, nval_per_col=4, skip_zero_bit=6, n_mcv=30, n_bins=60):\n",
    "    data = data_generation(skew, domain_size, correlation, column_size, nrows=nrows)\n",
    "    name = f\"toy_{skew}_{domain_size}_{correlation}_{column_size}\"\n",
    "    queries, cards = query_generation(data, name, num_sample, p, nval_per_col, skip_zero_bit)\n",
    "    BN = Bayescard_BN(name)\n",
    "    BN.build_from_data(data, n_mcv=n_mcv, n_bins=n_bins)\n",
    "    model_path = f'/home/ziniu.wzn/BN_checkpoints/synthetic/{name}.pkl'\n",
    "    pickle.dump(BN, open(model_path, 'wb'), pickle.HIGHEST_PROTOCOL)\n",
    "    BN.infer_algo = \"exact-jit\"\n",
    "    BN.init_inference_method()\n",
    "    latencies = []\n",
    "    q_errors = []\n",
    "    for query_no, query_str in enumerate(queries):\n",
    "        query = parse_query_single_table(query_str.strip(), BN)\n",
    "        cardinality_true = cards[query_no]\n",
    "        card_start_t = perf_counter()\n",
    "        cardinality_predict = BN.query(query)\n",
    "        card_end_t = perf_counter()\n",
    "        latency_ms = (card_end_t - card_start_t) * 1000\n",
    "        if cardinality_predict == 0 and cardinality_true == 0:\n",
    "            q_error = 1.0\n",
    "        elif np.isnan(cardinality_predict) or cardinality_predict == 0:\n",
    "            cardinality_predict = 1\n",
    "            q_error = max(cardinality_predict / cardinality_true, cardinality_true / cardinality_predict)\n",
    "        elif cardinality_true == 0:\n",
    "            cardinality_true = 1\n",
    "            q_error = max(cardinality_predict / cardinality_true, cardinality_true / cardinality_predict)\n",
    "        else:\n",
    "            q_error = max(cardinality_predict / cardinality_true, cardinality_true / cardinality_predict)\n",
    "        latencies.append(latency_ms)\n",
    "        q_errors.append(q_error)\n",
    "    for i in [50, 90, 95, 99, 100]:\n",
    "        print(f\"q-error {i}% percentile is {np.percentile(q_errors, i)}\")\n",
    "    print(f\"average latency is {np.mean(latencies)} ms\")\n",
    "    return q_errors, latencies\n",
    "\n",
    "def run_all_experiment():\n",
    "    parameters_to_explore = {\n",
    "        \"skew\": [0.1, 0.3, 0.6, 1.0, 1.3, 1.6, 2.0],\n",
    "        \"domain_size\": [10, 100, 1000, 10000],\n",
    "        \"correlation\": [0, 0.2, 0.4, 0.6, 0.8, 1.0],\n",
    "        \"column_num\": [2, 5, 10, 50, 100, 200]\n",
    "    }\n",
    "    \n",
    "    print(\"runing experiment on varying skewness: [0.1, 0.3, 0.6, 1.0, 1.3, 1.6, 2.0]}\")\n",
    "    print(\"controlled parameters are: domain_size = 100, correlation = 0.4, column_num = 10\")\n",
    "    skew_qerrors = []\n",
    "    for skew in parameters_to_explore[\"skew\"]:\n",
    "        print(\"============================================================\")\n",
    "        print(f\"Tesing skewness = {skew}\")\n",
    "        q_errors, latencies = train_one(skew, 100, 0.4, 10, nrows=1000000)\n",
    "        skew_qerrors.append(np.asarray(q_errors))\n",
    "    skew_qerrors = np.stack(skew_qerrors)\n",
    "    np.save(\"skew_qerrors\", skew_qerrors)\n",
    "    \n",
    "    print(\"============================================================\")\n",
    "    print(\"============================================================\")\n",
    "    \n",
    "    print(\"runing experiment on varying domain_size: [10, 100, 1000, 10000]\")\n",
    "    print(\"controlled parameters are: skewness = 1.0, correlation = 0.4, column_num = 10\")\n",
    "    domain_size_qerrors = []\n",
    "    for domain_size in parameters_to_explore[\"domain_size\"]:\n",
    "        print(\"============================================================\")\n",
    "        print(f\"Tesing domain_size = {domain_size}\")\n",
    "        q_errors, latencies = train_one(1, domain_size, 0.4, 10, nrows=1000000)\n",
    "        domain_size_qerrors.append(np.asarray(q_errors))\n",
    "    domain_size_qerrors = np.stack(domain_size_qerrors)\n",
    "    np.save(\"domain_size_qerrors\", domain_size_qerrors)\n",
    "    \n",
    "    print(\"============================================================\")\n",
    "    print(\"============================================================\")\n",
    "    \n",
    "    print(\"runing experiment on varying correlation: [0, 0.2, 0.4, 0.6, 0.8, 1.0]\")\n",
    "    print(\"controlled parameters are: skewness = 1.0, domain_size = 100, column_num = 10\")\n",
    "    correlation_qerrors = []\n",
    "    for correlation in parameters_to_explore[\"correlation\"]:\n",
    "        print(\"============================================================\")\n",
    "        print(f\"Tesing correlation = {correlation}\")\n",
    "        q_errors, latencies = train_one(1, 100, correlation, 10, nrows=1000000)\n",
    "        correlation_qerrors.append(np.asarray(q_errors))\n",
    "    correlation_qerrors = np.stack(correlation_qerrors)\n",
    "    np.save(\"correlation_qerrors\", correlation_qerrors)\n",
    "    \n",
    "    print(\"============================================================\")\n",
    "    print(\"============================================================\")\n",
    "    \n",
    "    print(\"runing experiment on varying column_num: [2, 5, 10, 50, 100, 200]\")\n",
    "    print(\"controlled parameters are: skewness = 1.0, domain_size = 100, correlation = 0.4\")\n",
    "    column_num_qerrors = []\n",
    "    for skew in parameters_to_explore[\"column_num\"]:\n",
    "        print(\"============================================================\")\n",
    "        print(f\"Tesing skewness = {column_num}\")\n",
    "        q_errors, latencies = train_one(column_num, 100, 0.4, 10, nrows=1000000)\n",
    "        column_num_qerrors.append(np.asarray(q_errors))\n",
    "    column_num_qerrors = np.stack(column_num_qerrors)\n",
    "    np.save(\"column_num_qerrors\", column_num_qerrors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_to_explore = {\n",
    "        \"skew\": [0.1, 0.3, 0.6, 1.0, 1.3, 1.6, 2.0],\n",
    "        \"domain_size\": [10, 100, 1000, 10000],\n",
    "        \"correlation\": [0, 0.2, 0.4, 0.6, 0.8, 1.0],\n",
    "        \"column_num\": [2, 5, 10, 50, 100, 200]\n",
    "    }\n",
    "    \n",
    "print(\"runing experiment on varying skewness: [0.1, 0.3, 0.6, 1.0, 1.3, 1.6, 2.0]}\")\n",
    "print(\"controlled parameters are: domain_size = 100, correlation = 0.4, column_num = 10\")\n",
    "skew_qerrors = []\n",
    "skip_zero_bit = [6, 6, 6, 4, 4, 2, 2]\n",
    "for i, skew in enumerate(parameters_to_explore[\"skew\"]):\n",
    "    print(\"============================================================\")\n",
    "    print(f\"Tesing skewness = {skew}\")\n",
    "    q_errors, latencies = train_one(skew, 100, 0.4, 10, skip_zero_bit=4)\n",
    "    skew_qerrors.append(np.asarray(q_errors))\n",
    "skew_qerrors = np.stack(skew_qerrors)\n",
    "np.save(\"skew_qerrors\", skew_qerrors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runing experiment on varying domain_size: [10, 100, 1000, 10000]\n",
      "controlled parameters are: skewness = 1.0, correlation = 0.4, column_num = 10\n",
      "============================================================\n",
      "Tesing domain_size = 10\n",
      "Discretizing table takes 4.263000249862671 secs\n",
      "Structure learning took 80.30355405807495 secs.\n",
      "done, parameter learning took 16.762226819992065 secs.\n",
      "q-error 50% percentile is 1.0267285596620204\n",
      "q-error 90% percentile is 1.169146723246805\n",
      "q-error 95% percentile is 1.2790444936273964\n",
      "q-error 99% percentile is 3.434182709651793\n",
      "q-error 100% percentile is 3.994890827880913\n",
      "average latency is 1.5486622229218483 ms\n",
      "============================================================\n",
      "Tesing domain_size = 100\n",
      "Discretizing table takes 9.00087285041809 secs\n",
      "Structure learning took 100.83552432060242 secs.\n",
      "done, parameter learning took 10.751103401184082 secs.\n",
      "q-error 50% percentile is 1.053352364062548\n",
      "q-error 90% percentile is 1.3124382992832175\n",
      "q-error 95% percentile is 1.553035476708711\n",
      "q-error 99% percentile is 2.157679786939669\n",
      "q-error 100% percentile is 2.6583254249069843\n",
      "average latency is 5.57666540145874 ms\n",
      "============================================================\n",
      "Tesing domain_size = 1000\n",
      "Discretizing table takes 40.915382385253906 secs\n",
      "Structure learning took 103.7759153842926 secs.\n",
      "done, parameter learning took 7.777698993682861 secs.\n",
      "q-error 50% percentile is 1.104096494740309\n",
      "q-error 90% percentile is 1.4745362157464699\n",
      "q-error 95% percentile is 1.7298533264686817\n",
      "q-error 99% percentile is 2.982376534354616\n",
      "q-error 100% percentile is 3.726316683170452\n",
      "average latency is 15.835421048104763 ms\n",
      "============================================================\n",
      "Tesing domain_size = 10000\n",
      "Discretizing table takes 5.250920295715332 secs\n",
      "Structure learning took 87.81491899490356 secs.\n",
      "done, parameter learning took 1.1467041969299316 secs.\n",
      "q-error 50% percentile is 3.5390185116625434\n",
      "q-error 90% percentile is 15.808225633905966\n",
      "q-error 95% percentile is 26.553081486239687\n",
      "q-error 99% percentile is 202.55476846810754\n",
      "q-error 100% percentile is 635.6997648226233\n",
      "average latency is 3.2133403792977333 ms\n"
     ]
    }
   ],
   "source": [
    "parameters_to_explore = {\n",
    "        \"skew\": [0.1, 0.3, 0.6, 1.0, 1.3, 1.6, 2.0],\n",
    "        \"domain_size\": [10, 100, 1000, 10000],\n",
    "        \"correlation\": [0, 0.2, 0.4, 0.6, 0.8, 1.0],\n",
    "        \"column_num\": [2, 5, 10, 50, 100, 200]\n",
    "    }\n",
    "print(\"runing experiment on varying domain_size: [10, 100, 1000, 10000]\")\n",
    "print(\"controlled parameters are: skewness = 1.0, correlation = 0.4, column_num = 10\")\n",
    "domain_size_qerrors = []\n",
    "for domain_size in parameters_to_explore[\"domain_size\"]:\n",
    "    print(\"============================================================\")\n",
    "    print(f\"Tesing domain_size = {domain_size}\")\n",
    "    if domain_size == 10000:\n",
    "        n_mcv=60\n",
    "        n_bins=120\n",
    "    else:\n",
    "        n_mcv=30\n",
    "        n_bins=60\n",
    "    q_errors, latencies = train_one(1, domain_size, 0.4, 10, nrows=1000000, skip_zero_bit=4, \n",
    "                                    n_mcv=n_mcv, n_bins=n_bins)\n",
    "    domain_size_qerrors.append(np.asarray(q_errors))\n",
    "domain_size_qerrors = np.stack(domain_size_qerrors)\n",
    "np.save(\"domain_size_qerrors\", domain_size_qerrors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/ziniu.wzn/BN_checkpoints/synthetic/toy_1_10000_0.4_10.pkl', 'rb') as f:\n",
    "    BN = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "skew = 1\n",
    "domain_size = 10000\n",
    "correlation = 0.4\n",
    "column_size = 10\n",
    "data = data_generation(skew, domain_size, correlation, column_size)\n",
    "name = f\"toy_{skew}_{domain_size}_{correlation}_{column_size}\"\n",
    "queries, cards = query_generation(data, name, 10, p=0.8, nval_per_col=4, skip_zero_bit=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = parse_query_single_table(queries[0].strip(), BN)\n",
    "cardinality_true = cards[0]\n",
    "BN.infer_algo = \"exact-jit\"\n",
    "BN.init_inference_method()\n",
    "cardinality_predict = BN.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
