{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "import sys\n",
    "sys.path.append('/home/ziniu.wzn/BayesCard')\n",
    "import pandas as pd\n",
    "import time\n",
    "import bz2\n",
    "import pickle\n",
    "import logging\n",
    "import ast\n",
    "\n",
    "#from DataPrepare.join_data_preparation import JoinDataPreparator\n",
    "from Models.pgmpy_BN import Pgmpy_BN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"/home/ziniu.wzn/DMV/DMV.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cols = []\n",
    "for col in data.columns:\n",
    "    if col in ['VIN', 'Zip', 'City', 'Make', 'Unladen Weight', 'Maximum Gross Weight', 'Passengers', 'Reg Valid Date', 'Reg Expiration Date', 'Color']:\n",
    "        data = data.drop(col, axis=1)\n",
    "    else:\n",
    "        new_cols.append(col.replace(\" \", \"_\"))\n",
    "data.columns = new_cols\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = time.time()\n",
    "BN = Pgmpy_BN('dmv')\n",
    "BN.build_from_data(data, algorithm=\"chow-liu\", max_parents=4, ignore_cols=['id'], sample_size=100000)\n",
    "print(time.time()-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from itertools import chain\n",
    "import copy\n",
    "from Pgmpy.inference import Inference\n",
    "from Pgmpy.factors import factor_product\n",
    "from Pgmpy.models import BayesianModel, JunctionTree\n",
    "from Pgmpy.inference.EliminationOrder import (\n",
    "    WeightedMinFill,\n",
    "    MinNeighbors,\n",
    "    MinFill,\n",
    "    MinWeight,\n",
    ")\n",
    "from Pgmpy.factors.discrete import TabularCPD\n",
    "\n",
    "class VariableElimination(object):\n",
    "    def __init__(self, model, probs=None):\n",
    "        model.check_model()\n",
    "        self.model = model\n",
    "        if probs is not None:\n",
    "            self.probs = probs\n",
    "        elif len(self.model.probs) != 0:\n",
    "            self.probs = model.probs\n",
    "        else:\n",
    "            self.probs = dict()\n",
    "\n",
    "        if isinstance(model, JunctionTree):\n",
    "            self.variables = set(chain(*model.nodes()))\n",
    "        else:\n",
    "            self.variables = model.nodes()\n",
    "\n",
    "        self.cardinality = {}\n",
    "        self.factors = defaultdict(list)\n",
    "\n",
    "        if isinstance(model, BayesianModel):\n",
    "            self.state_names_map = {}\n",
    "            for node in model.nodes():\n",
    "                cpd = model.get_cpds(node)\n",
    "                if isinstance(cpd, TabularCPD):\n",
    "                    self.cardinality[node] = cpd.variable_card\n",
    "                    cpd = cpd.to_factor()\n",
    "                for var in cpd.scope():\n",
    "                    self.factors[var].append(cpd)\n",
    "                self.state_names_map.update(cpd.no_to_name)\n",
    "\n",
    "        elif isinstance(model, JunctionTree):\n",
    "            self.cardinality = model.get_cardinality()\n",
    "\n",
    "            for factor in model.get_factors():\n",
    "                for var in factor.variables:\n",
    "                    self.factors[var].append(factor)\n",
    "        self.root = self.get_root()\n",
    "\n",
    "    def get_root(self):\n",
    "        \"\"\"Returns the network's root node.\"\"\"\n",
    "\n",
    "        def find_root(graph, node):\n",
    "            predecessor = next(self.model.predecessors(node), None)\n",
    "            if predecessor:\n",
    "                root = find_root(graph, predecessor)\n",
    "            else:\n",
    "                root = node\n",
    "            return root\n",
    "\n",
    "        return find_root(self, list(self.model.nodes)[0])\n",
    "\n",
    "    def steiner_tree(self, nodes):\n",
    "        \"\"\"Returns the minimal part of the tree that contains a set of nodes.\"\"\"\n",
    "        sub_nodes = set()\n",
    "\n",
    "        def walk(node, path):\n",
    "            if len(nodes) == 0:\n",
    "                return\n",
    "\n",
    "            if node in nodes:\n",
    "                sub_nodes.update(path + [node])\n",
    "                nodes.remove(node)\n",
    "\n",
    "            for child in self.model.successors(node):\n",
    "                walk(child, path + [node])\n",
    "\n",
    "        walk(self.root, [])\n",
    "        sub_graph = self.model.subgraph(sub_nodes)\n",
    "        sub_graph.cardinalities = defaultdict(int)\n",
    "        for node in sub_graph.nodes:\n",
    "            sub_graph.cardinalities[node] = self.model.cardinalities[node]\n",
    "        return sub_graph\n",
    "\n",
    "    def get_probs(self, attribute, values):\n",
    "        \"\"\"\n",
    "        Calculate Pr(attr in values) where values must be a list\n",
    "        \"\"\"\n",
    "        factor = self.probs[attribute]\n",
    "        values = [factor.get_state_no(attribute, no) for no in values]\n",
    "        return np.sum(factor.values[values])\n",
    "\n",
    "\n",
    "    def _get_working_factors(self, variables=[], evidence=None, return_probs=False, reduce=True):\n",
    "        \"\"\"\n",
    "        Uses the evidence given to the query methods to modify the factors before running\n",
    "        the variable elimination algorithm.\n",
    "        Parameters\n",
    "        ----------\n",
    "        evidence: dict\n",
    "            Dict of the form {variable: state}\n",
    "        Returns\n",
    "        -------\n",
    "        dict: Modified working factors.\n",
    "        \"\"\"\n",
    "\n",
    "        useful_var = copy.deepcopy(variables)\n",
    "        if evidence:\n",
    "            useful_var += list(evidence.keys())\n",
    "        sub_graph_model = self.steiner_tree(useful_var)\n",
    "        variables_sub_graph = set(sub_graph_model.nodes)\n",
    "\n",
    "        working_factors = dict()\n",
    "        for node in sub_graph_model.nodes:\n",
    "            working_factors[node] = set()\n",
    "            for factor in self.factors[node]:\n",
    "                if set(factor.variables).issubset(variables_sub_graph):\n",
    "                    working_factors[node].add((factor, None))\n",
    "\n",
    "        if return_probs:\n",
    "            probs = dict()\n",
    "        # Dealing with evidence. Reducing factors over it before VE is run.\n",
    "        if evidence and reduce:\n",
    "            for evidence_var in evidence:\n",
    "                for factor, origin in working_factors[evidence_var]:\n",
    "                    factor_reduced = factor.reduce(\n",
    "                        [(evidence_var, evidence[evidence_var])], inplace=False\n",
    "                    )\n",
    "                    if return_probs:\n",
    "                        factor_reduced.normalize()\n",
    "                        probs[evidence_var] = self.get_probs(evidence_var, evidence[evidence_var])\n",
    "                    for var in factor_reduced.scope():\n",
    "                        if var in working_factors:\n",
    "                            working_factors[var].remove((factor, origin))\n",
    "                            working_factors[var].add((factor_reduced, evidence_var))\n",
    "                if type(evidence[evidence_var]) != list:\n",
    "                    del working_factors[evidence_var]\n",
    "        if return_probs:\n",
    "            return working_factors, sub_graph_model, probs\n",
    "        return working_factors, sub_graph_model\n",
    "\n",
    "    def _get_elimination_order(\n",
    "            self, variables=None, evidence=None, model=None, elimination_order=\"minfill\", show_progress=False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Deals with all elimination order parameters given to _variable_elimination method\n",
    "        and returns a list of variables that are to be eliminated\n",
    "        Parameters\n",
    "        ----------\n",
    "        elimination_order: str or list\n",
    "        Returns\n",
    "        -------\n",
    "        list: A list of variables names in the order they need to be eliminated.\n",
    "        \"\"\"\n",
    "        if model is None:\n",
    "            model = self.model\n",
    "        if isinstance(model, JunctionTree):\n",
    "            all_variables = set(chain(*model.nodes()))\n",
    "        else:\n",
    "            all_variables = model.nodes()\n",
    "\n",
    "        if variables is None:\n",
    "            to_eliminate = set(all_variables)\n",
    "        else:\n",
    "            not_evidence_eliminate = []\n",
    "            if evidence is not None:\n",
    "                for key in evidence:\n",
    "                    if type(evidence[key]) != list:\n",
    "                        not_evidence_eliminate.append(key)\n",
    "            to_eliminate = (\n",
    "                    set(all_variables)\n",
    "                    - set(variables)\n",
    "                    - set(not_evidence_eliminate)\n",
    "            )\n",
    "\n",
    "        # Step 1: If elimination_order is a list, verify it's correct and return.\n",
    "        if hasattr(elimination_order, \"__iter__\") and (\n",
    "                not isinstance(elimination_order, str)\n",
    "        ):\n",
    "            if any(\n",
    "                    var in elimination_order\n",
    "                    for var in set(variables).union(\n",
    "                        set(evidence.keys() if evidence else [])\n",
    "                    )\n",
    "            ):\n",
    "                raise ValueError(\n",
    "                    \"Elimination order contains variables which are in\"\n",
    "                    \" variables or evidence args\"\n",
    "                )\n",
    "            else:\n",
    "                return elimination_order\n",
    "\n",
    "        # Step 2: If elimination order is None or a Markov model, return a random order.\n",
    "        elif (elimination_order is None) or (not isinstance(model, BayesianModel)):\n",
    "            return to_eliminate\n",
    "\n",
    "        # Step 3: If elimination order is a str, compute the order using the specified heuristic.\n",
    "        elif isinstance(elimination_order, str) and isinstance(\n",
    "                model, BayesianModel\n",
    "        ):\n",
    "            heuristic_dict = {\n",
    "                \"weightedminfill\": WeightedMinFill,\n",
    "                \"minneighbors\": MinNeighbors,\n",
    "                \"minweight\": MinWeight,\n",
    "                \"minfill\": MinFill,\n",
    "            }\n",
    "            elimination_order = heuristic_dict[elimination_order.lower()](\n",
    "                model\n",
    "            ).get_elimination_order(nodes=to_eliminate, show_progress=show_progress)\n",
    "            return elimination_order\n",
    "\n",
    "    def _variable_elimination(\n",
    "            self,\n",
    "            variables,\n",
    "            operation,\n",
    "            evidence=None,\n",
    "            elimination_order=\"minfill\",\n",
    "            joint=True,\n",
    "            show_progress=False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Implementation of a generalized variable elimination.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        variables: list, array-like\n",
    "            variables that are not to be eliminated.\n",
    "\n",
    "        operation: str ('marginalize' | 'maximize')\n",
    "            The operation to do for eliminating the variable.\n",
    "\n",
    "        evidence: dict\n",
    "            a dict key, value pair as {var: state_of_var_observed}\n",
    "            None if no evidence\n",
    "\n",
    "        elimination_order: str or list (array-like)\n",
    "            If str: Heuristic to use to find the elimination order.\n",
    "            If array-like: The elimination order to use.\n",
    "            If None: A random elimination order is used.\n",
    "        \"\"\"\n",
    "        # Step 1: Deal with the input arguments.\n",
    "        if isinstance(variables, str):\n",
    "            raise TypeError(\"variables must be a list of strings\")\n",
    "        if isinstance(evidence, str):\n",
    "            raise TypeError(\"evidence must be a list of strings\")\n",
    "\n",
    "        # Dealing with the case when variables is not provided.\n",
    "        if not variables:\n",
    "            all_factors = []\n",
    "            for factor_li in self.factors.values():\n",
    "                all_factors.extend(factor_li)\n",
    "            if joint:\n",
    "                return factor_product(*set(all_factors))\n",
    "            else:\n",
    "                return set(all_factors)\n",
    "\n",
    "        # Step 2: Prepare data structures to run the algorithm.\n",
    "        eliminated_variables = set()\n",
    "        # Get working factors and elimination order\n",
    "        # tic = time.time()\n",
    "        working_factors, sub_graph_model = self._get_working_factors(variables, evidence)\n",
    "        # toc = time.time()\n",
    "        # print(f\"getting working factors takes {toc-tic} secs\")\n",
    "        elimination_order = self._get_elimination_order(\n",
    "            variables, evidence, sub_graph_model, elimination_order, show_progress=show_progress\n",
    "        )\n",
    "        # print(f\"getting elimination orders takes {time.time()-toc} secs\")\n",
    "        # Step 3: Run variable elimination\n",
    "        if show_progress:\n",
    "            pbar = tqdm(elimination_order)\n",
    "        else:\n",
    "            pbar = elimination_order\n",
    "\n",
    "        for var in pbar:\n",
    "            #tic = time.time()\n",
    "            # print(var)\n",
    "            if show_progress:\n",
    "                pbar.set_description(\"Eliminating: {var}\".format(var=var))\n",
    "            # Removing all the factors containing the variables which are\n",
    "            # eliminated (as all the factors should be considered only once)\n",
    "            factors = [\n",
    "                factor\n",
    "                for factor, _ in working_factors[var]\n",
    "                if not set(factor.variables).intersection(eliminated_variables)\n",
    "            ]\n",
    "            phi = factor_product(*factors)\n",
    "            phi = getattr(phi, operation)([var], inplace=False)\n",
    "            del working_factors[var]\n",
    "            for variable in phi.variables:\n",
    "                if variable in working_factors:\n",
    "                    working_factors[variable].add((phi, var))\n",
    "            eliminated_variables.add(var)\n",
    "            # print(f\"eliminating {var} takes {time.time()-tic} secs\")\n",
    "\n",
    "        # Step 4: Prepare variables to be returned.\n",
    "        #tic = time.time()\n",
    "        final_distribution = set()\n",
    "        for node in working_factors:\n",
    "            for factor, origin in working_factors[node]:\n",
    "                if not set(factor.variables).intersection(eliminated_variables):\n",
    "                    final_distribution.add((factor, origin))\n",
    "        final_distribution = [factor for factor, _ in final_distribution]\n",
    "        # print(final_distribution)\n",
    "        # print(f\"the rest takes {time.time()-tic} secs\")\n",
    "        if joint:\n",
    "            if isinstance(self.model, BayesianModel):\n",
    "                return factor_product(*final_distribution).normalize(inplace=False)\n",
    "            else:\n",
    "                return factor_product(*final_distribution)\n",
    "        else:\n",
    "            query_var_factor = {}\n",
    "            for query_var in variables:\n",
    "                phi = factor_product(*final_distribution)\n",
    "                query_var_factor[query_var] = phi.marginalize(\n",
    "                    list(set(variables) - set([query_var])), inplace=False\n",
    "                ).normalize(inplace=False)\n",
    "            return query_var_factor\n",
    "\n",
    "    def query(\n",
    "            self,\n",
    "            variables,\n",
    "            evidence=None,\n",
    "            elimination_order=\"weightedminfill\",\n",
    "            joint=True,\n",
    "            show_progress=False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        variables: list\n",
    "            list of variables for which you want to compute the probability\n",
    "\n",
    "        evidence: dict\n",
    "            a dict key, value pair as {var: state_of_var_observed}\n",
    "            None if no evidence\n",
    "\n",
    "        elimination_order: list\n",
    "            order of variable eliminations (if nothing is provided) order is\n",
    "            computed automatically\n",
    "\n",
    "        joint: boolean (default: True)\n",
    "            If True, returns a Joint Distribution over `variables`.\n",
    "            If False, returns a dict of distributions over each of the `variables`.\n",
    "        \"\"\"\n",
    "        common_vars = set(evidence if evidence is not None else []).intersection(\n",
    "            set(variables)\n",
    "        )\n",
    "        if common_vars:\n",
    "            raise ValueError(\n",
    "                f\"Can't have the same variables in both `variables` and `evidence`. Found in both: {common_vars}\"\n",
    "            )\n",
    "\n",
    "        return self._variable_elimination(\n",
    "            variables=variables,\n",
    "            operation=\"marginalize\",\n",
    "            evidence=evidence,\n",
    "            elimination_order=elimination_order,\n",
    "            joint=joint,\n",
    "            show_progress=show_progress,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('clt.pkl', 'rb') as f:\n",
    "    BN = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BN.encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../Benchmark/DMV/query.sql\") as f:\n",
    "    queries = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ve = VariableElimination(BN.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries[119]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Evaluation.cardinality_estimation import parse_query_single_table\n",
    "tic = time.time()\n",
    "query_str = queries[119].split(\"||\")[0]\n",
    "BN.query(parse_query_single_table(query_str.strip(), BN))\n",
    "print(time.time()-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Evaluation.cardinality_estimation import parse_query_single_table\n",
    "eliminate = []\n",
    "from time import perf_counter\n",
    "def evaluate_cardinality(BN, query_path):\n",
    "    # read all queries\n",
    "    with open(query_path) as f:\n",
    "        queries = f.readlines()\n",
    "    latencies = []\n",
    "    q_errors = []\n",
    "    for query_no, query_str in enumerate(queries):\n",
    "        cardinality_true = int(query_str.split(\"||\")[-1])\n",
    "        query_str = query_str.split(\"||\")[0]\n",
    "        print(f\"Predicting cardinality for query {query_no}: {query_str}\")\n",
    "        \n",
    "        query = parse_query_single_table(query_str.strip(), BN)\n",
    "        card_start_t = perf_counter()\n",
    "        try:\n",
    "            cardinality_predict = BN.query(query)\n",
    "        except:\n",
    "            print(\"BN is wrong\")\n",
    "            eliminate.append(query_no+1)\n",
    "            continue\n",
    "        if cardinality_predict is None:\n",
    "            print(\"BN is wrong\")\n",
    "            eliminate.append(query_no+1)\n",
    "            continue\n",
    "        card_end_t = perf_counter()\n",
    "        latency_ms = (card_end_t - card_start_t) * 1000\n",
    "        if cardinality_predict == 0 and cardinality_true == 0:\n",
    "            q_error = 1.0\n",
    "        elif cardinality_predict == 0:\n",
    "            cardinality_predict = 1\n",
    "        elif cardinality_true == 0:\n",
    "            cardinality_true = 1\n",
    "        q_error = max(cardinality_predict / cardinality_true, cardinality_true / cardinality_predict)\n",
    "        print(f\"latency: {latency_ms} and error: {q_error}\")\n",
    "        if q_error > 50:\n",
    "            eliminate.append(query_no+1)\n",
    "        latencies.append(latency_ms)\n",
    "        q_errors.append(q_error)\n",
    "    return latencies, q_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latencies, q_errors = evaluate_cardinality(BN, \"../Benchmark/DMV/query.sql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_errors = np.asarray(q_errors)\n",
    "np.where(q_errors>100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eliminate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [50, 90, 95, 99, 100]:\n",
    "    print(np.percentile(q_errors, i))\n",
    "np.mean(latencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BN.model = BN.model.to_junction_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(BN, open('clt.pkl', 'wb'), pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "x = [22.13, 329, 90, 330, 1629, 9318, 1, 1]\n",
    "y = [52, 69, 46, 58, 345, 7230, 11300, 113000]\n",
    "types = [\"Chow-Liu\", \"Exact\", \"Greedy\", \"Junction\", \"SPN\", \"Naru\", \"1% Sample\", \"10% Sampling\"]\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x, y)\n",
    "\n",
    "ax.set_xlabel('Model training time (Secs in log scale)', fontsize=14)\n",
    "ax.set_ylabel('Model size (KB in log scale)', fontsize=14)\n",
    "ax.set_title('Model scalability of different methods', fontsize=14)\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "\n",
    "\n",
    "for i, txt in enumerate(types[0:4]):\n",
    "    if txt == \"Junction\":\n",
    "        xytext=(5,-5)\n",
    "    else:\n",
    "        xytext=(5,5)\n",
    "    ax.annotate(txt, (x[i], y[i]), xytext=xytext, textcoords='offset points', fontsize=10)\n",
    "    plt.scatter(x[i], y[i], color='blue', alpha=0.5)\n",
    "\n",
    "for i, txt in enumerate(types[4:]):\n",
    "    i += 4\n",
    "    ax.annotate(txt, (x[i], y[i]), xytext=xytext, textcoords='offset points', fontsize=10)\n",
    "    plt.scatter(x[i], y[i], color='red', alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
