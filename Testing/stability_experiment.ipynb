{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pareto\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.append('/home/ziniu.wzn/BayesCard')\n",
    "from Models.Bayescard_BN import Bayescard_BN\n",
    "from time import perf_counter\n",
    "from Evaluation.utils import parse_query\n",
    "from Evaluation.cardinality_estimation import parse_query_single_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize_series(s, domain_size):\n",
    "    n_invalid = len(s[s>=domain_size])\n",
    "    s = s[s<domain_size]\n",
    "    s = np.floor(s)\n",
    "    new_s = np.random.randint(domain_size, size=n_invalid)\n",
    "    s = np.concatenate((s, new_s))\n",
    "    return np.random.permutation(s)\n",
    "    \n",
    "def data_generation(skew, domain_size, correlation, column_size, nrows=1000000):\n",
    "    data = np.zeros((column_size, nrows))\n",
    "    for i in range(column_size):\n",
    "        if i == 0:\n",
    "            s = np.random.randint(domain_size, size=nrows)\n",
    "            data[i,:] = s\n",
    "            continue\n",
    "        s = pareto.rvs(b=skew, scale=1, size=nrows)\n",
    "        s = discretize_series(s, domain_size)\n",
    "        if i == 1:\n",
    "            selected_cols = [0]\n",
    "        else:\n",
    "            #num_selected_cols = max(np.random.randint(int(np.ceil(i*0.1))), 1)\n",
    "            num_selected_cols = 1\n",
    "            selected_cols = np.random.permutation(i)[0:num_selected_cols]\n",
    "        idx = np.random.permutation(nrows)[0:int(nrows*correlation)]\n",
    "        if len(idx) != 0:\n",
    "            selected_data = data[selected_cols, :]\n",
    "            selected_data = np.ceil(np.mean(selected_data, axis=0))\n",
    "            s[idx] = selected_data[idx]\n",
    "        assert len(np.unique(s)) <= domain_size, \"invalid domain\"\n",
    "        data[i,:] = s\n",
    "        \n",
    "    data = pd.DataFrame(data=data.transpose(), columns=[f\"attr{i}\" for i in range(column_size)])\n",
    "    return data\n",
    "\n",
    "def query_generation(data, table_name, num_sample=200, p=0.8, nval_per_col=4, skip_zero_bit=6):\n",
    "    queries = []\n",
    "    cards = []\n",
    "    for i in range(num_sample):\n",
    "        query, card = generate_single_query(data, table_name, p, nval_per_col, skip_zero_bit)\n",
    "        while query is None:\n",
    "            query, card = generate_single_query(data, table_name, p, nval_per_col, skip_zero_bit)\n",
    "        queries.append(query)\n",
    "        cards.append(card)\n",
    "    return queries, cards\n",
    "\n",
    "def generate_single_query(df, table_name, p=0.8, nval_per_col=4, skip_zero_bit=6):\n",
    "    \"\"\"\n",
    "    p, nval_per_col, and skip_zero_bit are controlling the true cardinality size. As we know smaller true card \n",
    "    generally leads to larger q-error, which will bias the experimental result, so we use this to control the \n",
    "    true card to be similar for all experiments.\n",
    "    \"\"\"\n",
    "    query = f\"SELECT COUNT(*) FROM {table_name} WHERE \"\n",
    "    execute_query = \"\"\n",
    "    column_names = df.columns\n",
    "    n_cols = 0\n",
    "    for i, col in enumerate(column_names):\n",
    "        a = np.random.choice([0,1], p=[p,1-p])\n",
    "        if a == 0:\n",
    "            index = np.random.choice(len(df), size=nval_per_col)\n",
    "            val = sorted(list(df[col].iloc[index]))\n",
    "            left_val = val[0]\n",
    "            right_val = val[-1]\n",
    "            if left_val == right_val:\n",
    "                sub_query = col + '==' + str(left_val) + ' and '\n",
    "                act_sub_query = col + ' = ' + str(left_val) + ' AND '\n",
    "            else:\n",
    "                if skip_zero_bit:\n",
    "                    left_val += skip_zero_bit\n",
    "                    right_val += skip_zero_bit\n",
    "                sub_query = str(left_val) + ' <= ' + col + ' <= ' + str(right_val) + ' and '\n",
    "                act_sub_query = col + ' >= ' + str(left_val) + ' AND ' + col + ' <= ' + str(right_val) + ' AND '\n",
    "            execute_query += sub_query\n",
    "            query += act_sub_query\n",
    "    if execute_query == \"\":\n",
    "        return None,  None\n",
    "    execute_query = execute_query[:-5]\n",
    "    query = query[:-5]\n",
    "    try:\n",
    "        card = len(df.query(execute_query))\n",
    "    except:\n",
    "        card = 0\n",
    "    if card==0:\n",
    "        return None, None\n",
    "    return query, card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one(skew, domain_size, correlation, column_size, nrows=1000000, num_sample=200, \n",
    "              p=0.8, nval_per_col=4, skip_zero_bit=6, rows_to_use=10000, n_mcv=30, n_bins=70):\n",
    "    data = data_generation(skew, domain_size, correlation, column_size, nrows=nrows)\n",
    "    name = f\"toy_{skew}_{domain_size}_{correlation}_{column_size}\"\n",
    "    queries, cards = query_generation(data, name, num_sample, p, nval_per_col, skip_zero_bit)\n",
    "    BN = Bayescard_BN(name)\n",
    "    BN.build_from_data(data, sample_size=rows_to_use, n_mcv=n_mcv, n_bins=n_bins)\n",
    "    model_path = f'/home/ziniu.wzn/BN_checkpoints/synthetic/{name}.pkl'\n",
    "    pickle.dump(BN, open(model_path, 'wb'), pickle.HIGHEST_PROTOCOL)\n",
    "    BN.infer_algo = \"exact-jit\"\n",
    "    BN.init_inference_method()\n",
    "    latencies = []\n",
    "    q_errors = []\n",
    "    for query_no, query_str in enumerate(queries):\n",
    "        query = parse_query_single_table(query_str.strip(), BN)\n",
    "        cardinality_true = cards[query_no]\n",
    "        card_start_t = perf_counter()\n",
    "        cardinality_predict = BN.query(query)\n",
    "        card_end_t = perf_counter()\n",
    "        latency_ms = (card_end_t - card_start_t) * 1000\n",
    "        if cardinality_predict == 0 and cardinality_true == 0:\n",
    "            q_error = 1.0\n",
    "        elif np.isnan(cardinality_predict) or cardinality_predict == 0:\n",
    "            cardinality_predict = 1\n",
    "            q_error = max(cardinality_predict / cardinality_true, cardinality_true / cardinality_predict)\n",
    "        elif cardinality_true == 0:\n",
    "            cardinality_true = 1\n",
    "            q_error = max(cardinality_predict / cardinality_true, cardinality_true / cardinality_predict)\n",
    "        else:\n",
    "            q_error = max(cardinality_predict / cardinality_true, cardinality_true / cardinality_predict)\n",
    "        latencies.append(latency_ms)\n",
    "        q_errors.append(q_error)\n",
    "    for i in [50, 90, 95, 99, 100]:\n",
    "        print(f\"q-error {i}% percentile is {np.percentile(q_errors, i)}\")\n",
    "    print(f\"average latency is {np.mean(latencies)} ms\")\n",
    "    return q_errors, latencies\n",
    "\n",
    "def run_all_experiment():\n",
    "    parameters_to_explore = {\n",
    "        \"skew\": [0.1, 0.3, 0.6, 1.0, 1.5, 2.0],\n",
    "        \"domain_size\": [10, 100, 500, 1000, 5000, 10000],\n",
    "        \"correlation\": [0, 0.2, 0.4, 0.6, 0.8, 1.0],\n",
    "        \"column_num\": [2, 5, 10, 50, 100, 200]\n",
    "    }\n",
    "    \n",
    "    print(\"runing experiment on varying skewness: [0.1, 0.3, 0.6, 1.0, 1.3, 1.6, 2.0]}\")\n",
    "    print(\"controlled parameters are: domain_size = 100, correlation = 0.4, column_num = 10\")\n",
    "    skew_qerrors = []\n",
    "    for skew in parameters_to_explore[\"skew\"]:\n",
    "        print(\"============================================================\")\n",
    "        print(f\"Tesing skewness = {skew}\")\n",
    "        q_errors, latencies = train_one(skew, 100, 0.4, 10, nrows=1000000)\n",
    "        skew_qerrors.append(np.asarray(q_errors))\n",
    "    skew_qerrors = np.stack(skew_qerrors)\n",
    "    np.save(\"skew_qerrors\", skew_qerrors)\n",
    "    \n",
    "    print(\"============================================================\")\n",
    "    print(\"============================================================\")\n",
    "    \n",
    "    print(\"runing experiment on varying domain_size: [10, 100, 1000, 10000]\")\n",
    "    print(\"controlled parameters are: skewness = 1.0, correlation = 0.4, column_num = 10\")\n",
    "    domain_size_qerrors = []\n",
    "    for domain_size in parameters_to_explore[\"domain_size\"]:\n",
    "        print(\"============================================================\")\n",
    "        print(f\"Tesing domain_size = {domain_size}\")\n",
    "        q_errors, latencies = train_one(1, domain_size, 0.4, 10, nrows=1000000)\n",
    "        domain_size_qerrors.append(np.asarray(q_errors))\n",
    "    domain_size_qerrors = np.stack(domain_size_qerrors)\n",
    "    np.save(\"domain_size_qerrors\", domain_size_qerrors)\n",
    "    \n",
    "    print(\"============================================================\")\n",
    "    print(\"============================================================\")\n",
    "    \n",
    "    print(\"runing experiment on varying correlation: [0, 0.2, 0.4, 0.6, 0.8, 1.0]\")\n",
    "    print(\"controlled parameters are: skewness = 1.0, domain_size = 100, column_num = 10\")\n",
    "    correlation_qerrors = []\n",
    "    for correlation in parameters_to_explore[\"correlation\"]:\n",
    "        print(\"============================================================\")\n",
    "        print(f\"Tesing correlation = {correlation}\")\n",
    "        q_errors, latencies = train_one(1, 100, correlation, 10, nrows=1000000)\n",
    "        correlation_qerrors.append(np.asarray(q_errors))\n",
    "    correlation_qerrors = np.stack(correlation_qerrors)\n",
    "    np.save(\"correlation_qerrors\", correlation_qerrors)\n",
    "    \n",
    "    print(\"============================================================\")\n",
    "    print(\"============================================================\")\n",
    "    \n",
    "    print(\"runing experiment on varying column_num: [2, 5, 10, 50, 100, 200]\")\n",
    "    print(\"controlled parameters are: skewness = 1.0, domain_size = 100, correlation = 0.4\")\n",
    "    column_num_qerrors = []\n",
    "    for column_num in parameters_to_explore[\"column_num\"]:\n",
    "        print(\"============================================================\")\n",
    "        print(f\"Tesing column_num = {column_num}\")\n",
    "        q_errors, latencies = train_one(1, 100, 0.4, column_num, nrows=1000000)\n",
    "        column_num_qerrors.append(np.asarray(q_errors))\n",
    "    column_num_qerrors = np.stack(column_num_qerrors)\n",
    "    np.save(\"column_num_qerrors\", column_num_qerrors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_to_explore = {\n",
    "        \"skew\": ([0.1, 0.3, 0.6, 1.0, 1.5, 2.0], [4,4,3,3,2,2]),\n",
    "        \"domain_size\": ([10, 100, 500, 1000, 5000, 10000], [4,4,4,4,4,4]),\n",
    "        \"correlation\": ([0, 0.2, 0.4, 0.6, 0.8, 1.0], [4,4,4,4,4,4]),\n",
    "        \"column_num\": ([2, 5, 10, 50, 100, 200], [6,6,4,2,0,0])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runing experiment on varying skewness: [0.1, 0.3, 0.6, 1.0, 1.3, 1.6, 2.0]}\n",
      "controlled parameters are: domain_size = 100, correlation = 0.4, column_num = 10\n",
      "============================================================\n",
      "Tesing skewness = 0.1\n",
      "Discretizing table takes 7.742928981781006 secs\n",
      "Structure learning took 1.8320200443267822 secs.\n",
      "done, parameter learning took 9.03601861000061 secs.\n",
      "q-error 50% percentile is 1.019442784641686\n",
      "q-error 90% percentile is 1.0479473167136624\n",
      "q-error 95% percentile is 1.0613888315042053\n",
      "q-error 99% percentile is 1.0814405254338593\n",
      "q-error 100% percentile is 1.1321119017840218\n",
      "average latency is 7.15815756469965 ms\n",
      "============================================================\n",
      "Tesing skewness = 0.3\n",
      "Discretizing table takes 6.3387229442596436 secs\n",
      "Structure learning took 1.875269889831543 secs.\n",
      "done, parameter learning took 10.549920320510864 secs.\n",
      "q-error 50% percentile is 1.0236816349813394\n",
      "q-error 90% percentile is 1.0696273039901292\n",
      "q-error 95% percentile is 1.0934889963221888\n",
      "q-error 99% percentile is 1.3826168392239966\n",
      "q-error 100% percentile is 1.8911336577282893\n",
      "average latency is 7.134841233491898 ms\n",
      "============================================================\n",
      "Tesing skewness = 0.6\n",
      "Discretizing table takes 6.559553146362305 secs\n",
      "Structure learning took 1.7351980209350586 secs.\n",
      "done, parameter learning took 6.776778936386108 secs.\n",
      "q-error 50% percentile is 1.036575030872259\n",
      "q-error 90% percentile is 1.134977714116273\n",
      "q-error 95% percentile is 1.259948753227547\n",
      "q-error 99% percentile is 1.8896035548675842\n",
      "q-error 100% percentile is 2.317647199172584\n",
      "average latency is 6.99317242950201 ms\n",
      "============================================================\n",
      "Tesing skewness = 1.0\n",
      "Discretizing table takes 6.007349967956543 secs\n",
      "Structure learning took 1.596487045288086 secs.\n",
      "done, parameter learning took 8.083250045776367 secs.\n",
      "q-error 50% percentile is 1.1043180778478954\n",
      "q-error 90% percentile is 1.398811888689499\n",
      "q-error 95% percentile is 1.4948210614757573\n",
      "q-error 99% percentile is 1.9445112304757401\n",
      "q-error 100% percentile is 2.2605658091232916\n",
      "average latency is 6.61078467965126 ms\n",
      "============================================================\n",
      "Tesing skewness = 1.5\n",
      "Discretizing table takes 5.759207010269165 secs\n",
      "Structure learning took 1.6630544662475586 secs.\n",
      "done, parameter learning took 8.37009882926941 secs.\n",
      "q-error 50% percentile is 1.1357369193843967\n",
      "q-error 90% percentile is 1.7308651842459823\n",
      "q-error 95% percentile is 2.3927487068086863\n",
      "q-error 99% percentile is 3.7395170037553473\n",
      "q-error 100% percentile is 4.282748985762344\n",
      "average latency is 6.345187351107597 ms\n",
      "============================================================\n",
      "Tesing skewness = 2.0\n",
      "Discretizing table takes 6.8039867877960205 secs\n",
      "Structure learning took 1.6548185348510742 secs.\n",
      "done, parameter learning took 6.096506357192993 secs.\n",
      "q-error 50% percentile is 1.134707953066179\n",
      "q-error 90% percentile is 1.8974625962048381\n",
      "q-error 95% percentile is 2.299608066467398\n",
      "q-error 99% percentile is 4.135997383992642\n",
      "q-error 100% percentile is 5.603540697650156\n",
      "average latency is 6.272032223641872 ms\n"
     ]
    }
   ],
   "source": [
    "\n",
    "skew = parameters_to_explore[\"skew\"][0]\n",
    "print(f\"runing experiment on varying skewness: {skew}\")\n",
    "print(\"controlled parameters are: domain_size = 100, correlation = 0.4, column_num = 10\")\n",
    "skew_qerrors = []\n",
    "for i in range(len(skew)):\n",
    "    skew = parameters_to_explore[\"skew\"][0][i]\n",
    "    skip_zero_bit = parameters_to_explore[\"skew\"][1][i]\n",
    "    print(\"============================================================\")\n",
    "    print(f\"Tesing skewness = {skew}\")\n",
    "    q_errors, latencies = train_one(skew, 100, 0.4, 10, skip_zero_bit=skip_zero_bit)\n",
    "    skew_qerrors.append(np.asarray(q_errors))\n",
    "skew_qerrors = np.stack(skew_qerrors)\n",
    "np.save(\"skew_qerrors\", skew_qerrors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runing experiment on varying domain_size: [10, 100, 500, 1000, 5000, 10000]\n",
      "controlled parameters are: skewness = 1.0, correlation = 0.4, column_num = 10\n",
      "============================================================\n",
      "Tesing domain_size = 10\n",
      "Discretizing table takes 3.2726800441741943 secs\n",
      "Structure learning took 1.0676326751708984 secs.\n",
      "done, parameter learning took 6.7733728885650635 secs.\n",
      "q-error 50% percentile is 1.0283518911708862\n",
      "q-error 90% percentile is 1.1685129334542326\n",
      "q-error 95% percentile is 1.2930413036356396\n",
      "q-error 99% percentile is 1.712292196714191\n",
      "q-error 100% percentile is 4.76605852747474\n",
      "average latency is 2.072511501610279 ms\n",
      "============================================================\n",
      "Tesing domain_size = 100\n",
      "Discretizing table takes 5.959499359130859 secs\n",
      "Structure learning took 1.753504991531372 secs.\n",
      "done, parameter learning took 8.807895421981812 secs.\n",
      "q-error 50% percentile is 1.0837036888106923\n",
      "q-error 90% percentile is 1.4317781550184194\n",
      "q-error 95% percentile is 1.5708856085079208\n",
      "q-error 99% percentile is 2.769807338265714\n",
      "q-error 100% percentile is 8.460473231771633\n",
      "average latency is 6.488492079079151 ms\n",
      "============================================================\n",
      "Tesing domain_size = 500\n",
      "Discretizing table takes 23.33048963546753 secs\n",
      "Structure learning took 2.0195324420928955 secs.\n",
      "done, parameter learning took 8.411479949951172 secs.\n",
      "q-error 50% percentile is 1.0160963766182154\n",
      "q-error 90% percentile is 1.0478534928267438\n",
      "q-error 95% percentile is 1.057862410144405\n",
      "q-error 99% percentile is 1.0730959872369863\n",
      "q-error 100% percentile is 1.0899560052747728\n",
      "average latency is 14.99289769679308 ms\n",
      "============================================================\n",
      "Tesing domain_size = 1000\n",
      "Discretizing table takes 45.705153703689575 secs\n",
      "Structure learning took 2.1902413368225098 secs.\n",
      "done, parameter learning took 6.520272970199585 secs.\n",
      "q-error 50% percentile is 1.0134748986117124\n",
      "q-error 90% percentile is 1.0302731774852636\n",
      "q-error 95% percentile is 1.0420704017353069\n",
      "q-error 99% percentile is 1.0551544907330108\n",
      "q-error 100% percentile is 1.12543585603545\n",
      "average latency is 26.182288862764835 ms\n",
      "============================================================\n",
      "Tesing domain_size = 5000\n",
      "Discretizing table takes 4.830335855484009 secs\n",
      "Structure learning took 1.6544575691223145 secs.\n",
      "done, parameter learning took 1.5560495853424072 secs.\n",
      "q-error 50% percentile is 22.295152306482258\n",
      "q-error 90% percentile is 48.41801385101184\n",
      "q-error 95% percentile is 55.35582547746574\n",
      "q-error 99% percentile is 80.83259287525144\n",
      "q-error 100% percentile is 97.04325810301928\n",
      "average latency is 3.6941003426909447 ms\n",
      "============================================================\n",
      "Tesing domain_size = 10000\n",
      "Discretizing table takes 3.9230000972747803 secs\n",
      "Structure learning took 1.5083117485046387 secs.\n",
      "done, parameter learning took 1.6477863788604736 secs.\n",
      "q-error 50% percentile is 25.55221574410843\n",
      "q-error 90% percentile is 45.79029852471275\n",
      "q-error 95% percentile is 49.86773403714526\n",
      "q-error 99% percentile is 65.44099834323313\n",
      "q-error 100% percentile is 83.07015514051173\n",
      "average latency is 5.0441670045256615 ms\n"
     ]
    }
   ],
   "source": [
    "domain_size = parameters_to_explore[\"domain_size\"][0]\n",
    "print(f\"runing experiment on varying domain_size: {domain_size}\")\n",
    "print(\"controlled parameters are: skewness = 1.0, correlation = 0.4, column_num = 10\")\n",
    "domain_size_qerrors = []\n",
    "for i in range(len(domain_size)):\n",
    "    domain_size = parameters_to_explore[\"domain_size\"][0][i]\n",
    "    skip_zero_bit = parameters_to_explore[\"domain_size\"][1][i]\n",
    "    print(\"============================================================\")\n",
    "    print(f\"Tesing domain_size = {domain_size}\")\n",
    "    q_errors, latencies = train_one(1, domain_size, 0.4, 10, nrows=1000000, skip_zero_bit=skip_zero_bit)\n",
    "    domain_size_qerrors.append(np.asarray(q_errors))\n",
    "domain_size_qerrors = np.stack(domain_size_qerrors)\n",
    "np.save(\"domain_size_qerrors\", domain_size_qerrors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runing experiment on varying correlation: [0, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
      "controlled parameters are: skewness = 1.0, domain_size = 100, column_num = 10\n",
      "============================================================\n",
      "Tesing correlation = 0\n",
      "Discretizing table takes 6.252589225769043 secs\n",
      "Structure learning took 105.05017137527466 secs.\n",
      "done, parameter learning took 7.225903749465942 secs.\n",
      "q-error 50% percentile is 1.5520722744737596\n",
      "q-error 90% percentile is 9.678739917888748\n",
      "q-error 95% percentile is 27.104876262849558\n",
      "q-error 99% percentile is 121.51581935214253\n",
      "q-error 100% percentile is 1030.289481648038\n",
      "average latency is 4.49909683316946 ms\n",
      "============================================================\n",
      "Tesing correlation = 0.2\n",
      "Discretizing table takes 6.2613749504089355 secs\n",
      "Structure learning took 103.8951849937439 secs.\n",
      "done, parameter learning took 7.121143341064453 secs.\n",
      "q-error 50% percentile is 1.427032086866471\n",
      "q-error 90% percentile is 3.5458052818169\n",
      "q-error 95% percentile is 7.945919027161713\n",
      "q-error 99% percentile is 13.551710462032924\n",
      "q-error 100% percentile is 30.27929504373782\n",
      "average latency is 6.053889058530331 ms\n",
      "============================================================\n",
      "Tesing correlation = 0.4\n",
      "Discretizing table takes 6.366618633270264 secs\n",
      "Structure learning took 99.20815086364746 secs.\n",
      "done, parameter learning took 7.264226198196411 secs.\n",
      "q-error 50% percentile is 1.1355503649568768\n",
      "q-error 90% percentile is 1.6718041931028917\n",
      "q-error 95% percentile is 2.135610450678466\n",
      "q-error 99% percentile is 4.422007175941709\n",
      "q-error 100% percentile is 7.212460791360774\n",
      "average latency is 6.345782019197941 ms\n",
      "============================================================\n",
      "Tesing correlation = 0.6\n",
      "Discretizing table takes 7.621510028839111 secs\n",
      "Structure learning took 92.44861006736755 secs.\n",
      "done, parameter learning took 11.809444904327393 secs.\n",
      "q-error 50% percentile is 1.0608901535399142\n",
      "q-error 90% percentile is 1.3096385927109737\n",
      "q-error 95% percentile is 1.4885563748208246\n",
      "q-error 99% percentile is 2.8765020694444465\n",
      "q-error 100% percentile is 6.856004788867383\n",
      "average latency is 13.23658186942339 ms\n",
      "============================================================\n",
      "Tesing correlation = 0.8\n",
      "Discretizing table takes 10.579801797866821 secs\n",
      "Structure learning took 56.72366523742676 secs.\n",
      "done, parameter learning took 10.785069227218628 secs.\n",
      "q-error 50% percentile is 1.0399363861108648\n",
      "q-error 90% percentile is 1.1877713889884893\n",
      "q-error 95% percentile is 1.4899678589780627\n",
      "q-error 99% percentile is 4.196064509354994\n",
      "q-error 100% percentile is 13.48243006365169\n",
      "average latency is 6.837006621062756 ms\n",
      "============================================================\n",
      "Tesing correlation = 1.0\n",
      "Discretizing table takes 7.578542232513428 secs\n",
      "Structure learning took 25.708008766174316 secs.\n",
      "done, parameter learning took 10.99835729598999 secs.\n",
      "q-error 50% percentile is 1.0\n",
      "q-error 90% percentile is 1.0000000000000002\n",
      "q-error 95% percentile is 1.0000000000000002\n",
      "q-error 99% percentile is 1.227143407635325\n",
      "q-error 100% percentile is 1.4871798384166581\n",
      "average latency is 6.301392279565334 ms\n"
     ]
    }
   ],
   "source": [
    "correlation = parameters_to_explore[\"correlation\"][0]\n",
    "print(f\"runing experiment on varying correlation: {correlation}\")\n",
    "print(\"controlled parameters are: skewness = 1.0, domain_size = 100, column_num = 10\")\n",
    "correlation_qerrors = []\n",
    "for i in range(len(correlation)):\n",
    "    domain_size = parameters_to_explore[\"correlation\"][0][i]\n",
    "    skip_zero_bit = parameters_to_explore[\"correlation\"][1][i]\n",
    "    print(\"============================================================\")\n",
    "    print(f\"Tesing correlation = {correlation}\")\n",
    "    q_errors, latencies = train_one(1, 100, correlation, 10, nrows=1000000, skip_zero_bit=skip_zero_bit)\n",
    "    correlation_qerrors.append(np.asarray(q_errors))\n",
    "correlation_qerrors = np.stack(correlation_qerrors)\n",
    "np.save(\"correlation_qerrors\", correlation_qerrors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runing experiment on varying column_num: [2, 5, 10, 50, 100, 200]\n",
      "controlled parameters are: skewness = 1.0, domain_size = 100, correlation = 0.4\n",
      "============================================================\n",
      "Tesing column number = 2\n",
      "Discretizing table takes 1.2706284523010254 secs\n",
      "Structure learning took 0.15340089797973633 secs.\n",
      "done, parameter learning took 0.4135129451751709 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziniu.wzn/anaconda3/envs/deepdb/lib/python3.7/site-packages/numpy/core/_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order, subok=True)\n",
      "/home/ziniu.wzn/anaconda3/envs/deepdb/lib/python3.7/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q-error 50% percentile is 1.0005554448935015\n",
      "q-error 90% percentile is 1.0128038297429311\n",
      "q-error 95% percentile is 1.0185697335971853\n",
      "q-error 99% percentile is 1.05202542813193\n",
      "q-error 100% percentile is 1.2479818364055857\n",
      "average latency is 1.2471112236380577 ms\n",
      "============================================================\n",
      "Tesing column number = 5\n",
      "Discretizing table takes 3.1535093784332275 secs\n",
      "Structure learning took 0.7785553932189941 secs.\n",
      "done, parameter learning took 0.7976157665252686 secs.\n",
      "q-error 50% percentile is 1.0216399616086824\n",
      "q-error 90% percentile is 1.067617156976904\n",
      "q-error 95% percentile is 1.1193217617163849\n",
      "q-error 99% percentile is 1.1963113954565479\n",
      "q-error 100% percentile is 1.330873324450124\n",
      "average latency is 4.372875913977623 ms\n",
      "============================================================\n",
      "Tesing column number = 10\n",
      "Discretizing table takes 6.959289312362671 secs\n",
      "Structure learning took 1.713876485824585 secs.\n",
      "done, parameter learning took 3.618835210800171 secs.\n",
      "q-error 50% percentile is 1.064602911691702\n",
      "q-error 90% percentile is 1.3131823292994318\n",
      "q-error 95% percentile is 1.479067259553217\n",
      "q-error 99% percentile is 2.3726884149010976\n",
      "q-error 100% percentile is 5.338522933531604\n",
      "average latency is 6.470584608614445 ms\n",
      "============================================================\n",
      "Tesing column number = 50\n",
      "Discretizing table takes 32.67996644973755 secs\n",
      "Structure learning took 9.468914031982422 secs.\n",
      "done, parameter learning took 133.28715634346008 secs.\n",
      "q-error 50% percentile is 1.1723956895647851\n",
      "q-error 90% percentile is 1.9859351300425652\n",
      "q-error 95% percentile is 2.581945909458687\n",
      "q-error 99% percentile is 6.386231264159564\n",
      "q-error 100% percentile is 10.834766815011461\n",
      "average latency is 13.303921483457088 ms\n",
      "============================================================\n",
      "Tesing column number = 100\n",
      "Discretizing table takes 65.30092430114746 secs\n",
      "Structure learning took 19.86301016807556 secs.\n",
      "done, parameter learning took 576.2290601730347 secs.\n",
      "q-error 50% percentile is 1.00194630125299\n",
      "q-error 90% percentile is 1.0077909724598588\n",
      "q-error 95% percentile is 1.011733390370449\n",
      "q-error 99% percentile is 1.0201064575269418\n",
      "q-error 100% percentile is 1.0877116860399525\n",
      "average latency is 19.08741470426321 ms\n",
      "============================================================\n",
      "Tesing column number = 200\n",
      "Discretizing table takes 128.44788575172424 secs\n",
      "Structure learning took 48.88461112976074 secs.\n",
      "done, parameter learning took 1907.015638589859 secs.\n",
      "q-error 50% percentile is 1.0021035595072134\n",
      "q-error 90% percentile is 1.0082066590565077\n",
      "q-error 95% percentile is 1.0124730961152513\n",
      "q-error 99% percentile is 1.022137306444787\n",
      "q-error 100% percentile is 1.0787420312300486\n",
      "average latency is 21.714445240795612 ms\n"
     ]
    }
   ],
   "source": [
    "column_num = parameters_to_explore[\"column_num\"][0]\n",
    "print(f\"runing experiment on varying column_num: {column_num}\")\n",
    "print(\"controlled parameters are: skewness = 1.0, domain_size = 100, correlation = 0.4\")\n",
    "column_num_qerrors = []\n",
    "p_list = [0.8, 0.8, 0.8, 0.2, 0.1, 0.05]\n",
    "for i in range(len(column_num)):\n",
    "    column_num = parameters_to_explore[\"column_num\"][0][i]\n",
    "    skip_zero_bit = parameters_to_explore[\"column_num\"][1][i]\n",
    "    print(\"============================================================\")\n",
    "    print(f\"Tesing column number = {column_num}\")\n",
    "    p = p_list[i]\n",
    "    q_errors, latencies = train_one(1, 100, 0.4, column_num, p=p, skip_zero_bit=skip_zero_bit, nrows=1000000)\n",
    "    column_num_qerrors.append(np.asarray(q_errors))\n",
    "column_num_qerrors = np.stack(column_num_qerrors)\n",
    "np.save(\"column_num_qerrors\", column_num_qerrors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
