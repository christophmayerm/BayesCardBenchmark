{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "import sys\n",
    "sys.path.append('/home/ziniu.wzn/BayesCard')\n",
    "import pandas as pd\n",
    "import time\n",
    "import bz2\n",
    "import pickle\n",
    "import logging\n",
    "import ast\n",
    "\n",
    "#from DataPrepare.join_data_preparation import JoinDataPreparator\n",
    "from Models.pgmpy_BN import Pgmpy_BN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('clt.pkl', 'rb') as f:\n",
    "    BN = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from itertools import chain\n",
    "import copy\n",
    "from Pgmpy.inference import Inference\n",
    "from Pgmpy.factors import factor_product\n",
    "from Pgmpy.models import BayesianModel, JunctionTree\n",
    "from Pgmpy.inference.EliminationOrder import (\n",
    "    WeightedMinFill,\n",
    "    MinNeighbors,\n",
    "    MinFill,\n",
    "    MinWeight,\n",
    ")\n",
    "from Pgmpy.factors.discrete import TabularCPD\n",
    "\n",
    "\n",
    "class VariableEliminationJIT(object):\n",
    "    def __init__(self, model, cpds, topological_order, topological_order_node, probs=None, root=True):\n",
    "        model.check_model()\n",
    "        self.cpds = cpds\n",
    "        self.topological_order = topological_order\n",
    "        self.topological_order_node = topological_order_node\n",
    "        self.model = model\n",
    "        if probs is not None:\n",
    "            self.probs = probs\n",
    "        else:\n",
    "            self.probs = dict()\n",
    "\n",
    "\n",
    "        self.variables = model.nodes()\n",
    "\n",
    "        self.cardinality = {}\n",
    "        self.factors = defaultdict(list)\n",
    "\n",
    "        if isinstance(model, BayesianModel):\n",
    "            self.state_names_map = {}\n",
    "            for node in model.nodes():\n",
    "                cpd = model.get_cpds(node)\n",
    "                if isinstance(cpd, TabularCPD):\n",
    "                    self.cardinality[node] = cpd.variable_card\n",
    "                    cpd = cpd.to_factor()\n",
    "                for var in cpd.scope():\n",
    "                    self.factors[var].append(cpd)\n",
    "                self.state_names_map.update(cpd.no_to_name)\n",
    "        else:\n",
    "            assert False, \"ExactCLT does not support models other than Discrete BN\"\n",
    "\n",
    "        if root:\n",
    "            self.root = self.get_root()\n",
    "\n",
    "    def get_root(self):\n",
    "        \"\"\"Returns the network's root node.\"\"\"\n",
    "\n",
    "        def find_root(graph, node):\n",
    "            predecessor = next(self.model.predecessors(node), None)\n",
    "            if predecessor:\n",
    "                root = find_root(graph, predecessor)\n",
    "            else:\n",
    "                root = node\n",
    "            return root\n",
    "\n",
    "        return find_root(self, list(self.model.nodes)[0])\n",
    "\n",
    "    def steiner_tree(self, nodes):\n",
    "        \"\"\"Returns the minimal part of the tree that contains a set of nodes.\"\"\"\n",
    "        sub_nodes = set()\n",
    "\n",
    "        def walk(node, path):\n",
    "            if len(nodes) == 0:\n",
    "                return\n",
    "\n",
    "            if node in nodes:\n",
    "                sub_nodes.update(path + [node])\n",
    "                nodes.remove(node)\n",
    "\n",
    "            for child in self.model.successors(node):\n",
    "                walk(child, path + [node])\n",
    "\n",
    "        walk(self.root, [])\n",
    "        sub_graph = self.model.subgraph(sub_nodes)\n",
    "        sub_graph.cardinalities = defaultdict(int)\n",
    "        for node in sub_graph.nodes:\n",
    "            sub_graph.cardinalities[node] = self.model.cardinalities[node]\n",
    "        return sub_graph\n",
    "\n",
    "\n",
    "    def _get_working_factors(self, query=None, return_probs=False, reduce=True):\n",
    "        \"\"\"\n",
    "        Uses the evidence given to the query methods to modify the factors before running\n",
    "        the variable elimination algorithm.\n",
    "        Parameters\n",
    "        ----------\n",
    "        evidence: dict\n",
    "            Dict of the form {variable: state}\n",
    "        Returns\n",
    "        -------\n",
    "        dict: Modified working factors.\n",
    "        \"\"\"\n",
    "        useful_var = list(query.keys())\n",
    "        sub_graph_model = self.steiner_tree(useful_var)\n",
    "\n",
    "        elimination_order = []\n",
    "        working_cpds = []\n",
    "        working_factors = dict()\n",
    "        for i, node in enumerate(self.topological_order_node[::-1]):\n",
    "            ind = len(self.topological_order_node)-i-1\n",
    "            if node in sub_graph_model.nodes:\n",
    "                elimination_order.append(node)\n",
    "                cpd = copy.deepcopy(self.cpds[ind])\n",
    "                working_cpds.append(cpd)\n",
    "                working_factors[node] = [cpd]\n",
    "\n",
    "        for node in sub_graph_model.nodes:\n",
    "            for cpd in working_cpds:\n",
    "                if node != cpd.variable and node in cpd.variables:\n",
    "                    working_factors[node].append(cpd)\n",
    "\n",
    "        return working_factors, sub_graph_model, elimination_order\n",
    "\n",
    "\n",
    "    def query(self, query, n_distinct=None):\n",
    "        \"\"\"\n",
    "        Compiles a ppl program into a fixed linear algebra program to speed up the inference\n",
    "        ----------\n",
    "        query: dict\n",
    "            a dict key, value pair as {var: state_of_var_observed}\n",
    "            None if no evidence\n",
    "        n_distinct: dict\n",
    "            a dict key, value pair as {var: probability of observed value in state}\n",
    "            This is for the case, where we bin the continuous or large domain so each state now contains many observed\n",
    "            value. Default to none, meaning no large domain.\n",
    "        \"\"\"\n",
    "        working_factors, sub_graph_model, elimination_order = self._get_working_factors(query)\n",
    "        for i, var in enumerate(elimination_order):\n",
    "            root_var = i == (len(elimination_order) - 1)\n",
    "            if len(working_factors[var]) == 1:\n",
    "                #leaf node in BN\n",
    "                if var in query:\n",
    "                    new_value = working_factors[var][0].values\n",
    "                    if n_distinct:\n",
    "                        new_value = np.dot(n_distinct[var], new_value[query[var]])\n",
    "                    else:\n",
    "                        new_value = np.sum(new_value[query[var]], axis=0)\n",
    "                    if root_var:\n",
    "                        return new_value\n",
    "                else:\n",
    "                    if root_var:\n",
    "                        return 1\n",
    "                    new_value = np.ones(working_factors[var][0].values.shape[-1])\n",
    "\n",
    "                assert len(new_value.shape) == 1, f\"unreduced variable {var}\"\n",
    "                working_factors[var][0].values = new_value\n",
    "            else:\n",
    "                if var in query:\n",
    "                    self_value = working_factors[var][0].values[query[var]]  #Pr(var|Parent(var))\n",
    "                    if n_distinct:\n",
    "                        self_value = (self_value.transpose() * n_distinct[var]).transpose()\n",
    "                    children_value = []\n",
    "                    #check if all children has been reduced\n",
    "                    for cpd in working_factors[var][1:]:\n",
    "                        #print(\"y\")\n",
    "                        child_value = cpd.values[query[var]]    #M(var) = Pr(child(var)|var)\n",
    "                        assert len(child_value.shape) == 1, \"unreduced children\"\n",
    "                        children_value.append(child_value)\n",
    "                    if len(children_value) == 1:\n",
    "                        children_value = children_value[0]\n",
    "                    else:\n",
    "                        #print(children_value)\n",
    "                        children_value = np.prod(np.stack(children_value), axis=0)\n",
    "                    if root_var:\n",
    "                        new_value = np.dot(self_value, children_value)\n",
    "                        return new_value\n",
    "                    new_value = np.dot(np.transpose(self_value), children_value)\n",
    "                else:\n",
    "                    self_value = working_factors[var][0].values  # Pr(var|Parent(var))\n",
    "                    children_value = []\n",
    "                    # check if all children has been reduced\n",
    "                    for cpd in working_factors[var][1:]:\n",
    "                        child_value = cpd.values  # M(var) = Pr(child(var)|var)\n",
    "                        assert len(child_value.shape) == 1, \"unreduced children\"\n",
    "                        children_value.append(child_value)\n",
    "                    if len(children_value) == 1:\n",
    "                        children_value = children_value[0]\n",
    "                    else:\n",
    "                        children_value = np.prod(np.stack(children_value), axis=0)\n",
    "                    if root_var:\n",
    "                        new_value = np.dot(self_value, children_value)\n",
    "                        return new_value\n",
    "                    new_value = np.dot(np.transpose(self_value), children_value)\n",
    "                assert len(new_value.shape) == 1, f\"unreduced variable {var}\"\n",
    "                working_factors[var][0].values = new_value\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_cpds_in_topological(BN):\n",
    "    cpds = BN.model.cpds\n",
    "    sampling_order = []\n",
    "    while len(sampling_order) < len(BN.structure):\n",
    "        for i, deps in enumerate(BN.structure):\n",
    "            if i in sampling_order:\n",
    "                continue  # already ordered\n",
    "            if all(d in sampling_order for d in deps):\n",
    "                sampling_order.append(i)\n",
    "    topological_order = sampling_order\n",
    "    topological_order_node = [BN.node_names[i] for i in sampling_order]\n",
    "    new_cpds = []\n",
    "    for n in topological_order_node:\n",
    "        for cpd in cpds:\n",
    "            if cpd.variable == n:\n",
    "                new_cpds.append(cpd)\n",
    "                break\n",
    "    assert len(cpds) == len(new_cpds)\n",
    "    return new_cpds, topological_order, topological_order_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/ziniu.wzn/BN_checkpoints/check_points/Census_chow-liu.pkl', 'rb') as f:\n",
    "    BN = pickle.load(f)\n",
    "BN.init_inference_method()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cpds, topological_order, topological_order_node = align_cpds_in_topological(BN)\n",
    "ve = VariableEliminationJIT(BN.model, cpds, topological_order, topological_order_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dAge',\n",
       " 'dIncome5',\n",
       " 'dIncome7',\n",
       " 'iRrelchld',\n",
       " 'iRspouse',\n",
       " 'iSchool',\n",
       " 'iYearsch',\n",
       " 'iYearwrk',\n",
       " 'iDisabl2',\n",
       " 'iLang1',\n",
       " 'iMarital',\n",
       " 'iMobility',\n",
       " 'dOccup',\n",
       " 'iRelat1',\n",
       " 'iRelat2',\n",
       " 'iRemplpar',\n",
       " 'iRlabor',\n",
       " 'iRownchld',\n",
       " 'iSubfam1',\n",
       " 'iSubfam2',\n",
       " 'iTmpabsnt',\n",
       " 'iWork89',\n",
       " 'iWorklwk',\n",
       " 'dAncstry1',\n",
       " 'dAncstry2',\n",
       " 'iDisabl1',\n",
       " 'iEnglish',\n",
       " 'iFertil',\n",
       " 'dHispanic',\n",
       " 'dIndustry',\n",
       " 'iLooking',\n",
       " 'iMeans',\n",
       " 'iMilitary',\n",
       " 'iMobillim',\n",
       " 'iPerscare',\n",
       " 'dPOB',\n",
       " 'dPoverty',\n",
       " 'iRagechld',\n",
       " 'iRiders',\n",
       " 'iRPOB',\n",
       " 'iRvetserv',\n",
       " 'iSept80',\n",
       " 'iSex',\n",
       " 'dTravtime',\n",
       " 'iVietnam',\n",
       " 'dWeek89',\n",
       " 'iWWII',\n",
       " 'dYrsserv',\n",
       " 'iAvail',\n",
       " 'iCitizen',\n",
       " 'iClass',\n",
       " 'dDepart',\n",
       " 'iFeb55',\n",
       " 'dHours',\n",
       " 'iImmigr',\n",
       " 'dIncome2',\n",
       " 'dIncome3',\n",
       " 'iKorean',\n",
       " 'iMay75880',\n",
       " 'iOthrserv',\n",
       " 'dPwgt1',\n",
       " 'dRearning',\n",
       " 'dRpincome',\n",
       " 'dHour89',\n",
       " 'dIncome1',\n",
       " 'dIncome4',\n",
       " 'dIncome6',\n",
       " 'dIncome8']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topological_order_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "BN.init_inference_method()\n",
    "#with open(\"../Benchmark/DMV/query.sql\") as f:\n",
    " #   queries = f.readlines()\n",
    "with open(\"/home/ziniu.wzn/Census/cardinality/query_one_side.sql\") as f:\n",
    "    queries = f.readlines()\n",
    "from Evaluation.cardinality_estimation import parse_query_single_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'iRelat2': [0], 'dTravtime': [1, 2, 0, 3], 'iYearsch': [11, 5, 10, 4, 8, 1, 7, 6, 2, 9, 3]}\n",
      "1490293\n",
      "{'iRelat2': [0], 'dTravtime': [3, 5, 0, 4], 'iYearsch': [1, 2, 0, 4, 7, 8, 6, 9, 15, 11, 14]}\n",
      "0.002078533172607422\n",
      "1490292.8364073704\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "query_str = queries[147].split(\"||\")[0]\n",
    "query_str = parse_query_single_table(query_str.strip(), BN)\n",
    "print(query_str)\n",
    "print(BN.query(copy.deepcopy(query_str)))\n",
    "query_str, n_distinct = BN.query_decoding(query_str, None)\n",
    "print(query_str)\n",
    "tic = time.time()\n",
    "a = ve.query(query_str, n_distinct)\n",
    "print(time.time()-tic)\n",
    "print(a*BN.nrows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1.]],\n",
       "\n",
       "       [[2., 2., 2., 2., 2., 2.],\n",
       "        [2., 2., 2., 2., 2., 2.],\n",
       "        [2., 2., 2., 2., 2., 2.],\n",
       "        [2., 2., 2., 2., 2., 2.],\n",
       "        [2., 2., 2., 2., 2., 2.]],\n",
       "\n",
       "       [[3., 3., 3., 3., 3., 3.],\n",
       "        [3., 3., 3., 3., 3., 3.],\n",
       "        [3., 3., 3., 3., 3., 3.],\n",
       "        [3., 3., 3., 3., 3., 3.],\n",
       "        [3., 3., 3., 3., 3., 3.]]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.ones((3,5,6)).transpose() * np.asarray([1,2,3])).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (3,) and (3,5,6) not aligned: 3 (dim 0) != 5 (dim 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-5cfaeeeadd45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (3,) and (3,5,6) not aligned: 3 (dim 0) != 5 (dim 1)"
     ]
    }
   ],
   "source": [
    "np.dot(np.asarray([1,2,3]), np.ones((3,5,6))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT COUNT(*) FROM DMV WHERE Registration_Class IN [PAS, COM, LTR, BOT, MOT, TRL, SRF, PSD, OMT, OMS, ATV, SNO] AND Body_Type IN [SUBN, 4DSD, PICK, 2DSD, BOAT, LTRL, MCY, TRLR, VAN, CONV, ATV, SNOW, DUMP, BUS, H/TR, UTIL, SEMI, TRAC, H/WH, DELV, FLAT, P/SH, STAK, TANK, MOPD, TAXI, TOW, SEDN, REFG] AND Scofflaw_Indicator == N\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_str = queries[147].split(\"||\")[0]\n",
    "print(query_str)\n",
    "parse_query_single_table(query_str.strip(), BN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(\"../Benchmark/DMV/query.sql\") as f:\n",
    " #   queries = f.readlines()\n",
    "with open(\"/home/ziniu.wzn/Census/cardinality/query_one_side.sql\") as f:\n",
    "    queries = f.readlines()\n",
    "\n",
    "res = []\n",
    "cards = []\n",
    "lat = []\n",
    "for q in queries:\n",
    "    query_str = q.split(\"||\")[0]\n",
    "    card = int(q.split(\"||\")[-1])\n",
    "    cards.append(card)\n",
    "    query_str = parse_query_single_table(query_str.strip(), BN)\n",
    "    #print(BN.query(query_str))\n",
    "    query_str, n_distinct = BN.query_decoding(query_str, None)\n",
    "    tic=time.time()\n",
    "    a = ve.query(query_str)\n",
    "    print(time.time()-tic)\n",
    "    lat.append(time.time()-tic)\n",
    "    res.append(a*BN.nrows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"average latency: {int(np.mean(lat)*100000)/100}ms\")\n",
    "pred = np.asarray(res)\n",
    "pred[pred<=1.0] = 1\n",
    "cards = np.asarray(cards)\n",
    "errors = np.maximum(np.divide(pred, cards), np.divide(pred, cards))\n",
    "for i in [50,90,95,99,100]:\n",
    "    print(f\"{i}% quantile: {np.percentile(errors, i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
